#!/usr/bin/env python3
# ==============================================================
#  Minimal working version of PulseGNLSESimulator
#  * keeps only the parts you need right now *
#    â€“ stores t_fwhm correctly
#    â€“ builds initial pulse that you can inspect
# ==============================================================

from __future__ import annotations

# å¤´éƒ¨ import
import torch
from torch import fft as tfft     # torch.fft åˆ«å
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(DEVICE)
import psutil
import numpy as np
import matplotlib.pyplot as plt
from math import factorial, sqrt, log, pi
from typing import Optional, Callable, Dict, Sequence
from scipy.interpolate import interp1d
import os
from scipy import fft as sp_fft      # åé¢å¦‚æœåˆ‡ CuPy æ”¹è¿™è¡Œå³å¯
import pathlib
WORKERS = os.cpu_count()             # ä¹Ÿå¯æ”¹æˆä½ æƒ³ç”¨çš„æ ¸æ•°
print(f'cpu counts: {WORKERS}')


class PulseGNLSESimulator:
    r"""
    Split-step (2 + 1)Dâ€‰+â€‰T Generalised NLSE simulator â€• **initialisation only**

    Parameters
    ----------
    # --- temporal ---
    pulse_energy : float
        Pulse energy [J].
    t_fwhm : float
        Intensity FWHM of the input pulse [s].
    nt : int
        Temporal sample count.
    t_window_factor : float
        Time-window length = ``factor Ã— t_fwhm``.

    # --- longitudinal ---
    L : float
        Physical propagation length [m].
    nz : int
        Split-step segments (uniform Î”z).
    compression_steps : int
        Points in quadratic-phase scan for RMS-TBP compression.

    # --- carrier & dispersion ---
    lambda0 : float
        Carrier wavelength [m].
    dispersion : dict[int, float]
        Must include Î²â‚ (order 1) and Î²â‚‚ (order 2) at minimum, units sâ¿ mâ»Â¹.

    # --- non-linear ---
    n2 : float | Sequence[tuple[float, float]]
        Kerr index - scalar or list of (Î», nâ‚‚) pairs (m, mÂ²/W).
    raman : dict | None
        Raman parameters ``{'f_R', 'tau1', 'tau2'}``; ``None`` â‡’ off.

    # --- transverse grid ---
    Nx, Ny : int
        Sample points in x- & y-directions.
    x_window, y_window : float
        Physical window size (full width) in x & y [m].
    beam_profile : dict
        Initial transverse shape.
        â€¢ ``{'shape':'gaussian','waist':w0}``
        â€¢ ``{'shape':'flat','radius':R}``
        â€¢ ``{'shape':'custom','func': callable}``

    # --- misc / optional ---
    alpha : float, default 0.0
        Linear attenuation coefficient Î± [mâ»Â¹].
    n0 : float, default 1.0
        Linear refractive index (=> kâ‚€ = nâ‚€ Ï‰â‚€/c).
    pml_width : float, default 0.0
        Absorbing boundary (PML) thickness [m]; 0 â‡’ none.
    device : {'cpu','cuda','auto'}, default 'cpu'
        Execution backend selector.
    retarded_frame : bool, default False
        ``False`` â‡’ laboratory frame (Î²â‚ active);
        ``True``  â‡’ moving frame (Î²â‚ dropped internally).
    """
    # ------------------------------------------------------------------
    def __init__(
        self,
        *,
        pulse_energy: float,
        t_fwhm: float,
        nt: int,
        t_window_factor: float,
        L: float,
        nz: int,
        compression_steps: int,
        lambda0: float,
        dispersion: dict[int, float],
        n2,
        Nx: int,
        Ny: int,
        x_window: float,
        y_window: float,
        beam_profile: dict,
        raman: dict | None = None,
        alpha: float = 0.0,
        noise: dict | None = None,
        n0: float = 1.0,
        pml_width: float = 0.0,
        device: str = "cpu",
        retarded_frame: bool = False,
        generate_A_in: bool = True,
        
    ) -> None:

        # -------- store basic inputs ----------
        Rc= None
        self.pulse_energy   = float(pulse_energy)
        self.t_fwhm         = float(t_fwhm)        # â˜… stored for later use
        self.nt             = int(nt)
        self.t_window_factor= float(t_window_factor)
        self.beam_profile   = beam_profile
        self.noise_cfg = noise or {}

        
        # ---------- basic constants ----------------------------------
        self.c = 2.99792458e8                # [m sâ»Â¹]
        self.lambda0 = float(lambda0)
        self.omega0  = 2*pi*self.c/self.lambda0
        self.k0      = n0*self.omega0/self.c
        self.n0      = n0
        self.alpha   = float(alpha)
        self.device  = device.lower()
        

        
        # ---------- store propagation settings -----------------------
        self.L  = float(L)
        self.nz = int(nz)
        self.dz = self.L / self.nz
        self.retarded = bool(retarded_frame)

        # ---------- dispersion dict validation -----------------------
        self.dispersion = {}
        for order, beta in dispersion.items():
            self.dispersion[int(order)] = float(beta)
        if not self.retarded and 1 not in self.dispersion:
            raise ValueError("laboratory frame requires Î²â‚ (order 1) in dispersion")
        if 2 not in self.dispersion:
            raise ValueError("dispersion must at least contain Î²â‚‚ (order 2)")

        self.n0      = n0
        R = ((self.n0 - 1)/(self.n0 + 1))**2  # ~0.035
        self.T_surf = 1 - R

        # ---------- time grid ----------------------------------------
        self.nt = int(nt)
        T_lim   = t_window_factor * t_fwhm / 2          # Â± limit
        self.T  = torch.linspace(-T_lim, +T_lim, self.nt, dtype=torch.float32, device=DEVICE)
        self.dT = float(self.T[1] - self.T[0])
        # frequency / angular-frequency grids
        f = tfft.fftfreq(self.nt, d=self.dT, device=DEVICE)   # â˜… ä¸å† fftshift
        self.omega   = 2 * torch.pi * f              # [rad/s]
        self.domega  = float(self.omega[1] - self.omega[0])
        self.freq    = self.omega / (2*torch.pi)

        # ---------- transverse grid ----------------------------------
        self.Nx, self.Ny = int(Nx), int(Ny)
        self.x = torch.linspace(-x_window/2, +x_window/2, self.Nx,
                                dtype=torch.float32, device=DEVICE)
        self.y = torch.linspace(-y_window/2, +y_window/2, self.Ny,
                                dtype=torch.float32, device=DEVICE)
        self.dx = float(self.x[1] - self.x[0])
        self.dy = float(self.y[1] - self.y[0])
        # 2-D mesh & k-space grids
        self.X, self.Y = torch.meshgrid(self.x, self.y, indexing='ij')
        kx = 2 * torch.pi * tfft.fftfreq(self.Nx, d=self.dx, device=DEVICE)   # â˜…
        ky = 2 * torch.pi * tfft.fftfreq(self.Ny, d=self.dy, device=DEVICE)   # â˜…
        self.Kx, self.Ky = torch.meshgrid(kx, ky, indexing='ij')              # â˜… å–æ¶ˆ fftshift
        self.K_perp2 = self.Kx**2 + self.Ky**2

        self._scale_fft_t   = (self.dT / np.sqrt(2*np.pi))
        self._scale_ifft_t  = (self.nt * self.domega / np.sqrt(2*np.pi))
        self._scale_fft_xy  = (self.dx * self.dy) / (2*np.pi)
        self._scale_ifft_xy = (2*np.pi) / (self.dx * self.dy)
        
        # è½¬æˆä¸æ•°æ®åŒ dtype/device çš„æ ‡é‡å¼ é‡ï¼ˆé¿å…æ¯æ¬¡æ„å»ºï¼‰
        to_scalar = lambda v, dev=DEVICE: torch.tensor(v, dtype=torch.float32, device=dev)
        self._scale_fft_t   = to_scalar(self._scale_fft_t)
        self._scale_ifft_t  = to_scalar(self._scale_ifft_t)
        self._scale_fft_xy  = to_scalar(self._scale_fft_xy)
        self._scale_ifft_xy = to_scalar(self._scale_ifft_xy)


        # ---------- Î³(Ï‰) å…¨é¢‘çŸ¢é‡ ------------------------------------
        omega_abs = self.omega + self.omega0          # â† æå‰æ”¾åˆ° if å¤–

        if np.isscalar(n2):
            n2_vec = float(n2) * torch.ones_like(self.omega, device=DEVICE)
        else:
            lam_pts, n2_pts = np.asarray(n2).T
            n2_interp = interp1d(lam_pts, n2_pts, kind='linear',
                                  bounds_error=False,
                                  fill_value=(n2_pts[0], n2_pts[-1]))
            lam_grid = (2*torch.pi*self.c / omega_abs.cpu()).numpy()
            n2_vec = torch.from_numpy(n2_interp(lam_grid)).to(DEVICE)

        # Î³(Ï‰) â€” ä¸ fftshift åçš„ Ï‰ å¯¹é½ï¼Œå½¢çŠ¶ (1,1,Nt)
        self.gamma_w = (omega_abs * n2_vec / self.c).to(torch.float32)[None, None, :].to(torch.complex64)
        
        # ä¸­å¿ƒé¢‘ç‡é‚£ä¸€ç‚¹ç•™ä½œè¯Šæ–­ / B-integral
        self.gamma0  = float(self.gamma_w[0, 0, 0])

        # ---------- Raman preparation --------------------------------
        self.raman_on = raman is not None
        if self.raman_on:
            self.f_R  = float(raman["f_R"])
            self.tau1 = float(raman["tau1"])
            self.tau2 = float(raman["tau2"])
            # build normalised Raman kernel h_R(t)
            h_R   = torch.zeros_like(self.T) 
            mask = self.T >= 0
            coeff = (self.tau1**2 + self.tau2**2) / (self.tau1*self.tau2**2)
            h_R[mask] = coeff \
                        * torch.exp(-self.T[mask]/self.tau2) \
                        * torch.sin(self.T[mask]/self.tau1)
            norm = torch.trapz(h_R, self.T)
            h_R  = h_R / norm

            # â‘£ é¢‘åŸŸè¡¨ç¤º â€”â€” ç›´æ¥è°ƒç”¨å·²æ”¹å¥½çš„ torch ç‰ˆ self.fft_t
            _, H_R = self.fft_t(h_R)          # è¿”å› torch.complex64, cuda
            self.H_R_omega = H_R              # (Nt,) â†’ åç»­å¹¿æ’­æ—¶ä¼šåŠ ç»´
        if self.raman_on:
            print('raman on')


        
        # ---- stepped film placeholder (route A geometry only at this step) ----
        self.stepped_film: SteppedFilm | None = None
        
        # === Route A: å®šä¹‰å‚è€ƒä»‹è´¨(çœŸç©º)ä¸è–„è†œææ–™çš„å‚æ•° ===
        self.n_ref = 1.0                               # å‚è€ƒä»‹è´¨ = çœŸç©º
        self.k0_ref = self.omega0 / self.c             # k_ref = Ï‰0/c
        
        # å‚è€ƒè‰²æ•£ Î²^refï¼ˆçœŸç©ºï¼‰ï¼šÎ²1 = 1/cï¼›mâ‰¥2 ~ 0ï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼‰
        beta_ref = {1: 1.0/self.c}
        # è‹¥ä½ æƒ³æ˜¾å¼ç½®é›¶ä»¥æ¸…æ™°ï¼š for m in (2,3,4): beta_ref[m] = 0.0
        
        # è–„è†œææ–™è‰²æ•£ Î²^film ä½¿ç”¨ç°æœ‰ self.dispersionï¼ˆå·²åœ¨ä¸Šæ–‡æ ¡éªŒï¼‰
        beta_film = dict(self.dispersion)
        
        self.materials = dict(
            n_ref=self.n_ref,
            n_film=self.n0,
            beta_ref=beta_ref,
            beta_film=beta_film,
            alpha_ref=0.0,
            alpha_film=self.alpha,
        )


        

        # ---------- pre-compute half-step linear operator -------------
        # build Ï‰ & k_perp^2 arrays with broadcast shape (Nx,Ny,Nt)
        # â€”â€” é¢„è®¡ç®—çº¿æ€§åŠæ­¥çš„ä¸¤ä¸ªå› å­ï¼ˆåˆ†ç¦»å­˜å‚¨ï¼‰ â€”â€”
        # ç©ºé—´å› å­ï¼šä¾èµ– kx, ky
        self.Lxy_half = torch.exp(
            (-1j * self.K_perp2 / (2 * self.k0)) * (self.dz / 2)
        ).to(torch.complex64)                               # (Nx, Ny)
        
        # æ—¶é—´å› å­ï¼šä¾èµ– Ï‰ ä¸ Î²_mã€Î±
        Om = self.omega.to(torch.complex128)               # (Nt,)
        disp_phase = torch.zeros_like(Om, dtype=torch.complex128)
        for order, beta in self.dispersion.items():
            if self.retarded and order == 1:
                continue
            disp_phase += beta * (Om**order) * ((-1)**order) / factorial(order)
        
        self.Lw_half = torch.exp(
            (1j * disp_phase - self.alpha/2) * (self.dz / 2)
        ).to(torch.complex64)                               # (Nt,)

        
        
        # â€”â€” ä¸€æ¬¡æ€§é¢„è®¡ç®—ï¼Œåç»­é‡å¤ä½¿ç”¨ï¼ˆé¿å…æ¯æ­¥ dtype è½¬æ¢ & é‡å»ºï¼‰ â€”â€”
        self.gamma_w_even128 = (0.5*(self.gamma_w + torch.flip(self.gamma_w, dims=[-1]))).to(torch.complex128)
        if self.raman_on:
            self.H_R_omega128 = self.H_R_omega.to(torch.complex128)
        self.omega128 = self.omega.to(torch.complex128)
        
        # ---------- other bookkeeping --------------------------------
        if generate_A_in:
            self.A_in_gpu = self._generate_initial_pulse(
            chirp = 0.0,                             # å¯é€‰åˆå§‹å•å•¾
            extra_spatial_phase = lambda X,Y:        # å¯é€‰æ³¢å‰æ›²ç‡
                0.5 * self.k0 / Rc * (X**2 + Y**2) if Rc is not None else 0.0
            )
            self.A_in = self.A_in_gpu.cpu()                     # åªä¿ç•™ CPU å‰¯æœ¬
            del self.A_in_gpu                                    # ç«‹åˆ»é‡Šæ”¾ GPU æ˜¾å­˜
            torch.cuda.empty_cache()
        else:
            self.A_in = None
            
        # ---- è®°å½•åˆå§‹ k-ç©ºé—´ä¸€ç»´ FWHMï¼ˆkx/kyï¼‰ï¼Œä¾èµ–ç°æœ‰ diagnose() ----
        # è¯´æ˜ï¼šdiagnose(domain='kspace') åœ¨å†…éƒ¨ä¼šè°ƒç”¨ self.fft_xy(self.A)ï¼Œ
        # å› æ­¤è¿™é‡Œä¸´æ—¶æŠŠ self.A æŒ‡å‘ GPU ç‰ˆæœ¬çš„ A_inï¼Œè®¡ç®—å®Œå†è¿˜åŸå¹¶æ¸…ç©ºç¼“å­˜ã€‚
        try:
            A_saved = getattr(self, "A", None)
            self.A  = self.A_in.to(self.T.device, non_blocking=True)  # ä¸´æ—¶ GPU æ‹·è´
            diag_k  = self.diagnose(domain='kspace', metrics=('fwhm',), save_raw=False)
            self.kx_fwhm0 = float(diag_k['metrics']['fwhm']['kx'])   # [m^-1]
            self.ky_fwhm0 = float(diag_k['metrics']['fwhm']['ky'])   # [m^-1]
            self.k_fwhm0  = 0.5 * (self.kx_fwhm0 + self.ky_fwhm0)    # ä½œä¸ºä»£è¡¨é‡
        finally:
            self.A = A_saved
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        print(self.k_fwhm0)
        self.k_fwhm0=600


        # === 2.1 ç©ºé—´å‚è€ƒç³»æ•°ï¼š_coef_xy_ref (Nx,Ny) ===
        self._coef_xy_ref = (-1j * self.K_perp2 / (2.0 * self.k0_ref)).to(torch.complex64)
        
        # === 2.2 æ—¶é—´å‚è€ƒç³»æ•°ï¼š_coef_w_ref (Nt,) ===
        Om128   = self.omega.to(torch.complex128)
        disp_ref = torch.zeros_like(Om128, dtype=torch.complex128)
        for m, beta in self.materials["beta_ref"].items():
            if self.retarded and m == 1:   # å…±åŠ¨åæ ‡ä¸‹è·³è¿‡ Î²1_ref
                continue
            disp_ref += beta * ((-1)**m) * (Om128**m) / factorial(m)
        self._coef_w_ref = (1j * disp_ref - self.materials["alpha_ref"]/2.0).to(torch.complex64)


        
        # === 2.3 å¢é‡ç³»æ•°ï¼š_coef_w_inc (Nt,) â€”â€” æ³¨æ„ Î”Î²1 å¿…é¡»ä¿ç•™ï¼ ===
        # å¢é‡é¢‘åŸŸå¤šé¡¹å¼ï¼ˆå« m=1 åœ¨å†…çš„æ‰€æœ‰ Î”Î²mï¼‰
        disp_inc = torch.zeros_like(Om128, dtype=torch.complex128)
        orders = set(self.materials["beta_film"].keys()) | set(self.materials["beta_ref"].keys())
        for m in orders:
            if m >= 1:  # m=1 è‡ªåŠ¨åŒ…å« Î”Î²1ï¼›m>=2 ä¸ºæ›´é«˜é˜¶
                bf = float(self.materials["beta_film"].get(m, 0.0))
                br = float(self.materials["beta_ref"].get(m, 0.0))  # çœŸç©ºï¼šbeta_ref[1]=1/c, å…¶ä½™â‰ˆ0
                disp_inc += (bf - br) * ((-1)**m) * (Om128**m) / factorial(m)
        
        # Î”Î²0 ä¸ Î”Î±
        delta_beta0 = (self.omega0 / self.c) * (self.materials["n_film"] - self.materials["n_ref"])

        delta_alpha = self.materials["alpha_film"] - self.materials["alpha_ref"]
        
        # æœ€ç»ˆå¢é‡ç³»æ•°ï¼ˆcomplex64 ä»¥çœæ˜¾å­˜/ç®—åŠ›ï¼‰
        self._coef_w_inc = (1j * (delta_beta0 + disp_inc) - delta_alpha/2.0).to(torch.complex64)

        # === è‹¥æ—¶é—´å‚è€ƒç®—å­æ’ç­‰ï¼Œå°±å¯ä»¥åœ¨ä¼ æ’­æ—¶è·³è¿‡ä¸¤æ¬¡ t-FFT ===
        with torch.no_grad():
            # è¿™é‡Œçš„é˜ˆå€¼å¯ä»¥æŒ‰æ•°å€¼éœ€æ±‚å¾®è°ƒ
            self._time_ref_is_identity = (torch.max(torch.abs(self._coef_w_ref)).item() < 1e-14)


        
        # å·¥ä½œå‰¯æœ¬åªåœ¨ GPU
        self.A = self.A_in     # ä¼ æ’­å‰æ¬å› GPU
        self.A_out = None  
        self.B_running=0
        self.B_log= []
        self.spatial_w=[]
        self.spatial_freq_w=[]


    # ---------- temporal ----------
    @torch.inference_mode()
    def fft_t(self, A_xt):
        A_w = tfft.fft(A_xt, dim=-1)
        A_w.mul_(self._scale_fft_t)
        return self.omega, A_w
    
    @torch.inference_mode()
    def ifft_t(self, A_w):
        A_t = tfft.ifft(A_w, dim=-1)
        A_t.mul_(self._scale_ifft_t)
        return self.T, A_t

        
    # ---------- spatial ----------
    @torch.inference_mode()
    def fft_xy(self, A_xyt):
        A_k = tfft.fftn(A_xyt, dim=(0,1))
        A_k.mul_(self._scale_fft_xy)
        return self.Kx, self.Ky, A_k
    
    @torch.inference_mode()
    def ifft_xy(self, A_kxt):
        A_xy = tfft.ifftn(A_kxt, dim=(0,1))
        A_xy.mul_(self._scale_ifft_xy)
        return self.X, self.Y, A_xy


    def fft_xy_test(self, A_xyt: torch.Tensor):
        device = A_xyt.device
        torch.cuda.synchronize(device)
        torch.cuda.reset_peak_memory_stats(device)
    
        before_alloc = torch.cuda.memory_allocated(device)
        before_reserved = torch.cuda.memory_reserved(device)
        t0 = time.time()
    
        # åˆ†æ­¥åšï¼Œä»¥ä¾¿åœ¨å…³é”®ç¯èŠ‚æ‰“ç‚¹
        A1 = tfft.ifftshift(A_xyt, dim=(0,1))
        mid1 = torch.cuda.memory_allocated(device)
        A2 = tfft.fftn(A1, dim=(0,1))
        del A1
        torch.cuda.empty_cache()
        mid2 = torch.cuda.memory_allocated(device)
        A3 = tfft.fftshift(A2, dim=(0,1))
        del A2
        torch.cuda.empty_cache()
        result = A3 * (self.dx * self.dy) / (2*torch.pi)
        del A3
        torch.cuda.empty_cache()
        torch.cuda.synchronize(device)
        after_alloc = torch.cuda.memory_allocated(device)
        peak_alloc = torch.cuda.max_memory_allocated(device)
        t1 = time.time()
    
        print(f"[FFT_xy]  time: {t1-t0:.3f}s")
        print(f"  before alloc:   {human(before_alloc)}")
        print(f"  after ifftshift: {human(mid1)}")
        print(f"  after fftn:     {human(mid2)}")
        print(f"  after fftshift: {human(after_alloc)}")
        print(f"  peak alloc:     {human(peak_alloc)}")
        print(f"  reserved:       {human(torch.cuda.memory_reserved(device))}")
        print("â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“")
    
        return self.Kx, self.Ky, result


    # =================================================================
    #  Build stepped-film geometry (ROUTE A, geometry only)
    # =================================================================
    def set_stepped_film_from_annuli(
        self,
        *,
        inner_radii: Sequence[float],     # len K, meters; inner_radii[0]=None for full disk
        dL_list: Sequence[float],         # len K, meters (layer thicknesses)
        nz_per_layer: Sequence[int] | int,  # len K or scalar
        dr_fwhm: float,                   # edge smoothing FWHM [m]
        # --- æ–°å¢ï¼šè‡ªåŠ¨å…‰é˜‘å‚æ•° ---
        aperture_radius: float | None = None,   # è‹¥ None ä¸” aperture_factor ä¸ä¸º None â†’ è‡ªåŠ¨è®¾ä¸º aperture_factor * w
        aperture_factor: float | None = 3.0,    # ç¼ºçœ = 3wï¼›è®¾ä¸º None åˆ™ä¸åŠ å¤–å…‰é˜‘
        aperture_fwhm: float | None = None,     # è½¯è¾¹ç¼˜ FWHMï¼›é»˜è®¤è·Ÿ dr_fwhm ä¸€æ ·
        names: Sequence[str] | None = None,
        to_device: torch.device | None = None,
    ) -> SteppedFilm:
        """
        Construct self.stepped_film with K layers.
        è‹¥ aperture_radius is None ä¸” aperture_factor ç»™å‡ºï¼Œåˆ™è‡ªåŠ¨ä½¿ç”¨ aperture_factor * w ä½œä¸ºå¤–å…‰é˜‘ã€‚
        """
        K = len(dL_list)
        assert len(inner_radii) == K, "inner_radii and dL_list must match length"
    
        if isinstance(nz_per_layer, int):
            nz_list = [int(nz_per_layer)] * K
        else:
            assert len(nz_per_layer) == K, "nz_per_layer must be int or length-K"
            nz_list = [int(z) for z in nz_per_layer]
    
        dev = to_device or self.X.device
    
        # ------- è‡ªåŠ¨å¤–å…‰é˜‘ï¼ˆé»˜è®¤ 3wï¼‰ -------
        if aperture_radius is None and (aperture_factor is not None):
            w = self._infer_beam_radius()
            aperture_radius = float(aperture_factor) * float(w)
        # ä¸è¦è¶…è¿‡ç½‘æ ¼è¾¹ç•Œ
        if aperture_radius is not None:
            r_grid = float(min(self.x.abs().max(), self.y.abs().max()))
            aperture_radius = float(min(aperture_radius, r_grid * 0.98))
        if aperture_fwhm is None:
            aperture_fwhm = dr_fwhm
    
        # ------- ç”Ÿæˆå„å±‚æ©è†œ -------
        masks = make_annular_masks(
            self.X.to(dev), self.Y.to(dev),
            inner_radii=inner_radii,
            dr_fwhm=dr_fwhm,
            aperture_radius=aperture_radius,
            aperture_fwhm=aperture_fwhm,
        )
    
        layers: List[LayerSpec] = []
        for j in range(K):
            nm = "" if names is None else str(names[j])
            layers.append(LayerSpec(dL=float(dL_list[j]), nz=int(nz_list[j]),
                                    mask=masks[j].to(torch.float32), name=nm))
        film = SteppedFilm(layers)
        self.stepped_film = film
        return film

    def _infer_beam_radius(self, fallback: float | None = None) -> float | None:
        """è¿”å› 1/e^2 åŠå¾„ wï¼ˆç±³ï¼‰ã€‚ä¼˜å…ˆä» beam_profile['waist']ï¼Œå¦åˆ™å›é€€åˆ°ç½‘æ ¼å°ºå¯¸ä¼°è®¡ã€‚"""
        try:
            shape = str(self.beam_profile.get("shape", "gaussian")).lower()
            if shape in ("gaussian", "supergauss", "supergaussian"):
                w = float(self.beam_profile["waist"])
                if w > 0:
                    return w
        except Exception:
            pass
        if fallback is not None:
            return fallback
        # å›é€€ï¼šå–ç½‘æ ¼åŠå®½çš„ 0.4 å€ï¼Œä¿è¯ä¸ä¼šè¶…è¿‡çª—å£
        r_grid = float(min(self.x.abs().max(), self.y.abs().max()))
        return 0.4 * r_grid
    

    
    # =================================================================
    #  GPU ç‰ˆ _generate_initial_pulse
    # =================================================================
    @torch.inference_mode()
    def _generate_initial_pulse(
        self,
        *,
        chirp: float = 0.0,
        extra_spatial_phase: Optional[
            Callable[[torch.Tensor, torch.Tensor], torch.Tensor]
        ] = None,
    ) -> torch.Tensor:
        """
        Build A(x,y,t) on GPU so that âˆ­|A|Â² dxâ€¯dyâ€¯dt = pulse_energy.
        Returns
        -------
        torch.Tensor
            dtype = complex64, device = DEVICE, shape (Nx,â€¯Ny,â€¯Nt)
        """
    
        # ---------- temporal envelope (normalised) ------------------
        t0  = self.t_fwhm / (2 * torch.sqrt(torch.log(torch.tensor(2.0))))
        A_t = torch.exp(-(self.T**2) / (2 * t0**2))          # float32
        if chirp != 0.0:
            A_t = A_t.to(torch.complex64) * torch.exp(1j * chirp * self.T**2)
        else:
            A_t = A_t.to(torch.complex64)
        int_t = torch.sqrt(torch.tensor(torch.pi)) * t0       # âˆ«|A_t|Â² dt  (float32)
    
        # ---------- spatial envelope (normalised) -------------------
        shape = self.beam_profile.get("shape", "gaussian").lower()
        if shape == "import":
            # 1) å–å‡ºç°æˆåœº
            field = self.beam_profile["field"]          # numpy / torch / â€¦
            if isinstance(field, np.ndarray):
                A0 = torch.from_numpy(field)
            else:
                A0 = field
            if not torch.is_tensor(A0):
                raise TypeError("beam_profile['field'] must be numpy array or torch tensor")
    
            # 2) å½¢çŠ¶æ ¡éªŒ
            if A0.shape != (self.Nx, self.Ny, self.nt):
                raise ValueError(
                    f"Imported field shape {A0.shape} â‰  simulator grid {(self.Nx, self.Ny, self.nt)}"
                )
    
            # 3) æ¬åˆ°ç›®æ ‡è®¾å¤‡ & dtype
            A0 = A0.to(dtype=torch.complex64, device=DEVICE).clone()
    
            # 4) å¯é€‰èƒ½é‡é‡æ ‡å®šï¼ˆé»˜è®¤é‡æ ‡ï¼‰
            if self.beam_profile.get("renorm", True):
                E_file = torch.trapz(
                    torch.trapz(torch.trapz(torch.abs(A0) ** 2, self.T), self.y), self.x
                )
                scale = torch.sqrt(self.pulse_energy / E_file)
                A0 *= scale
    
            return A0   # â† ç›´æ¥è¿”å›ï¼Œåé¢åˆ†æ”¯å…¨éƒ¨è·³è¿‡    

        if shape == "flat":
            R = float(self.beam_profile["radius"])
            A_xy = torch.where(
                self.X**2 + self.Y**2 <= R**2, 1.0, 0.0
            ).to(torch.complex64)
    
        elif shape == "gaussian":
            w0   = float(self.beam_profile["waist"])
            A_xy = torch.exp(-(self.X**2 + self.Y**2) / w0**2).to(torch.complex64)
    
        elif shape in {"supergauss", "supergaussian"}:
            w0   = float(self.beam_profile["waist"])
            n    = int(self.beam_profile.get("order", 8))
            A_xy = torch.exp(
                -((self.X**2 + self.Y**2) / w0**2) ** (n / 2)
            ).to(torch.complex64)
    
        elif shape == "custom":
            if "func" in self.beam_profile:
                A_xy = self.beam_profile["func"](self.X, self.Y).to(torch.complex64)
            elif "amp" in self.beam_profile:
                amp_np = np.asarray(self.beam_profile["amp"], dtype=np.complex64)
                if amp_np.shape != self.X.shape:
                    raise ValueError("beam_profile['amp'] shape must match (Nx,Ny)")
                A_xy = torch.from_numpy(amp_np).to(self.X.device)
            else:
                raise ValueError("custom shape needs 'func' or 'amp' key")
    
        else:
            raise ValueError("shape must be flat | gaussian | supergauss | custom")
    
        # ---------- optional noise ----------------------------------
        noise = self.noise_cfg
        if noise:
            sig_amp   = float(noise.get("sigma_amp",   0.0))
            sig_phase = float(noise.get("sigma_phase", 0.0))
    
            if sig_amp > 0.0:
                A_xy *= 1.0 + torch.randn_like(A_xy.real) * sig_amp
            if sig_phase > 0.0:
                A_xy *= torch.exp(1j * torch.randn_like(A_xy.real) * sig_phase)
    
            # ---- coherent lowâ€‘order modes --------------------------
            if "coherent_modes" in noise and noise.get("coh_level", 0.0) > 0.0:
                Nm  = int(noise["coherent_modes"])
                lvl = float(noise["coh_level"])
                coh_map = torch.zeros_like(A_xy.real)
                rng = torch.Generator(device=DEVICE)
                for _ in range(Nm):
                    kx = torch.empty(1, device=DEVICE).uniform_(0.5, 3.0)
                    ky = torch.empty(1, device=DEVICE).uniform_(0.5, 3.0)
                    phi = torch.empty(1, device=DEVICE).uniform_(0, 2*torch.pi)
                    kx *= torch.pi / self.beam_profile["waist"]
                    ky *= torch.pi / self.beam_profile["waist"]
                    coh_map += torch.cos(kx * self.X + ky * self.Y + phi)
                coh_map -= coh_map.mean()
                coh_map  = lvl * coh_map / coh_map.std()
                A_xy *= 1.0 + coh_map
    
        # ---------- normalise spatial power -------------------------
        area_xy = torch.trapz(
            torch.trapz(torch.abs(A_xy)**2, self.y), self.x
        )                               # float32
        A_xy /= torch.sqrt(area_xy)     # now âˆ«âˆ«|A_xy|Â² dxâ€¯dy = 1
    
        if extra_spatial_phase is not None:
            phase = extra_spatial_phase(self.X, self.Y)
            if not torch.is_tensor(phase):                       # â† æ–°å¢
                phase = torch.zeros_like(self.X) + phase         # â† æ–°å¢
            A_xy *= torch.exp(1j * phase)

        # ---------- scale to desired pulse energy -------------------
        A_scale = torch.sqrt(self.pulse_energy / int_t)      # scalar (float32 / cpu ok)
        A0 = A_scale * (A_xy[..., None] * A_t[None, None, :])
        return A0.to(torch.complex64)        # shape (Nx,Ny,Nt), gpu



    def _Lxy_ref_half(self, dz_half: float):
        return torch.exp(self._coef_xy_ref * float(dz_half))          # (Nx,Ny)

    def _Lw_ref_half(self, dz_half: float):
        return torch.exp(self._coef_w_ref * float(dz_half))           # (Nt,)
    
    def _amp_increment_half(self, dz_half: float):
        return torch.exp(self._coef_w_inc * float(dz_half))           # (Nt,)

    @torch.inference_mode()
    def _space_ref_half(self, dz_half: float) -> None:
        """å‚è€ƒä»‹è´¨çš„ç©ºé—´åŠæ­¥ï¼šA(x,y,t) -> A(kx,ky,t) ä¹˜ä»¥ exp(_coef_xy_ref*dz/2) -> å›æ¥ã€‚"""
        # (x,y,t) â†’ (kx,ky,t)
        _, _, A_k = self.fft_xy(self.A)
        # ä¹˜ä»¥ç©ºé—´åŠæ­¥ï¼ˆä¸ t æ— å…³ï¼Œå¹¿æ’­åˆ° Ntï¼‰
        self.A = None
        torch.cuda.empty_cache()
        A_k *= self._Lxy_ref_half(dz_half)[..., None]  # (Nx,Ny,1)
        # (kx,ky,t) â†’ (x,y,t)
        _, _, self.A = self.ifft_xy(A_k)
    

    @torch.inference_mode()
    def _time_ref_half(self, dz_half: float, *, eps: float = 1e-14) -> None:
        """å‚è€ƒä»‹è´¨çš„æ—¶é—´åŠæ­¥ï¼šA(x,y,t) -> A(x,y,Ï‰) ä¹˜ä»¥ exp(_coef_w_ref*dz/2) -> å›æ¥ã€‚"""
        # å¯é€‰ï¼šå¦‚æœæœ¬åŠæ­¥å‡ ä¹æ’ç­‰ï¼ˆä¾‹å¦‚çœŸç©º + retarded_frame å»æ‰ Î²1ï¼‰ï¼Œå°±è·³è¿‡
        max_mag = torch.max(torch.abs(self._coef_w_ref * float(dz_half))).item()
        if max_mag < eps:
            return
        # (x,y,t) â†’ (x,y,Ï‰)
        _, A_w = self.fft_t(self.A)
        # ä¹˜ä»¥æ—¶é—´åŠæ­¥ï¼ˆå¹¿æ’­åˆ° Nx,Nyï¼‰
        self.A = None
        torch.cuda.empty_cache()
        A_w *= self._Lw_ref_half(dz_half)[None, None, :]  # (1,1,Nt)
        # (x,y,Ï‰) â†’ (x,y,t)
        _, self.A = self.ifft_t(A_w)


    @torch.inference_mode()
    def _apply_incremental_linear_half(
        self,
        Mj: torch.Tensor,        # (Nx,Ny) float32
        dz_half: float,
        *,
        eps: float = 0.0,        # å¯é€‰ï¼šç®—å­è¿‘ä¼¼æ’ç­‰æ—¶è·³è¿‡
    ) -> None:
        """
        Route-Aï¼šå¢é‡çº¿æ€§åŠæ­¥ï¼ˆä¸åˆ‡å—ï¼Œä¸€æ¬¡æ€§åœ¨æ•´å— (Nx,Ny,Nt) ä¸Šæ‰§è¡Œï¼‰ã€‚
        é¢‘åŸŸæ›´æ–°å…¬å¼ï¼ˆå¯¹æ¯ä¸ª Î©ï¼‰ï¼š
            Ã‚_new = Ã‚_old * [ (1-M) + M * amp(Î©) ]
                  = Ã‚_old + M * (amp(Î©)-1) * Ã‚_old
        å…¶ä¸­ amp(Î©) = exp(_coef_w_inc(Î©) * dz_half)ï¼ŒM = Mj(x,y) âˆˆ [0,1] è½¯æ©è†œã€‚
        """
    
        dev = self.T.device
    
        # 1) é¢„å¤‡ï¼šæ©è†œä¸â€œå¢é‡ç®—å­â€
        Mj = Mj.to(device=dev, dtype=torch.float32, non_blocking=True)   # (Nx,Ny)
        amp = self._amp_increment_half(dz_half)                           # (Nt,) complex64
    
        # å¯é€‰ï¼šå¦‚æœæœ¬åŠæ­¥å‡ ä¹æ’ç­‰ï¼Œç›´æ¥è·³è¿‡ï¼ˆèŠ‚çœ FFT ä¸æ˜¾å­˜ï¼‰
        if eps > 0.0:
            if torch.max(torch.abs((amp - 1.0))).item() < eps:
                return
    
        # 2) (x,y,t) â†’ (x,y,Ï‰)
        _, A_w = self.fft_t(self.A)             # A_w: (Nx,Ny,Nt) complex64
        # é‡Šæ”¾æ—¶åŸŸ Aï¼Œé™ä½å³°å€¼æ˜¾å­˜ï¼ˆå…³é”®ä¸€æ­¥ï¼‰
        self.A = None
        torch.cuda.empty_cache()
    
        # 3) é¢‘åŸŸå†…â€œåªå¯¹è†œå†…åŒºåŸŸä¹˜ ampâ€ï¼Œå¯¹è†œå¤–ä¿æŒä¸å˜
        #    å†™æˆâ€œæ‹¼å›â€å½¢å¼ä»¥å‡å°‘ä¸­é—´å¤§å¼ é‡ï¼š
        #    A_w_masked = A_w * amp
        #    A_w        = (1-M)*A_w + M*A_w_masked
        A_w_masked = A_w * amp[None, None, :]                         # (Nx,Ny,Nt)
        # å…ˆæŠŠ A_w å˜æˆ â€œå¤–éƒ¨ä»½â€ï¼š (1-M)*A_w
        A_w.mul_((1.0 - Mj)[..., None].to(torch.float32))             # in-place
        # æŠŠè†œå†…éƒ¨åˆ†åŠ å›ï¼š M * A_w_masked
        A_w_masked.mul_(Mj[..., None].to(torch.float32))              # in-place
        A_w.add_(A_w_masked)                                          # in-place ç´¯åŠ 
        del A_w_masked
        torch.cuda.empty_cache()
        # 4) (x,y,Ï‰) â†’ (x,y,t)
        _, self.A = self.ifft_t(A_w)                                     # (Nx,Ny,Nt)e()
 


    
    @torch.inference_mode()
    def _kerr_step_with_film(self, dz: float, Mj: torch.Tensor, *,
                             tile_x: int = 32, empty_cache_every: int = 0):
        """
        Kerr/Raman/shock éçº¿æ€§æ­¥ï¼ˆæŒ‰ xâ€‘tile æµå¼ï¼‰ã€‚æœ¬å‡½æ•°å‡å®šéçº¿æ€§ä»…åœ¨è–„è†œåŒºåŸŸç”Ÿæ•ˆï¼Œ
        å› æ­¤å§‹ç»ˆä½¿ç”¨ä¼ å…¥çš„è½¯æ©è†œ Mj(x,y) âˆˆ [0,1]ï¼š
            A *= exp(i * [Mj * Ï†(t)] * dz)         ï¼ˆSPM çº¯ç›¸ä½ï¼‰
            A -= Mj * (Î³0/Ï‰0) * dz * âˆ‚t[A(t) N(t)] ï¼ˆshock æ ¡æ­£ï¼‰
        """
        with torch.no_grad():
            # --- å¸¸é‡/ç¼“å­˜ ---
            Nx, Ny, Nt = self.Nx, self.Ny, self.nt
            dev = self.T.device
    
            # Shock ç³»æ•°ï¼šk_shock = (Î³0/Ï‰0) * dz
            k_shock = (self.gamma0 / self.omega0) * dz
    
            # é¢‘åŸŸæ ¸ï¼ˆå·²åœ¨ __init__ é‡Œå‡†å¤‡å¥½ï¼Œå¤æ‚æ­¥éª¤é‡Œç”¨ complex128 æ›´ç¨³ï¼‰
            gamma_even = self.gamma_w_even128   # (1,1,Nt) complex128ï¼Œå¶å¯¹ç§° Î³(Ï‰)
            omega128   = self.omega128          # (Nt,)    float64/complex128
            H_R        = self.H_R_omega128 if self.raman_on else None
    
            # æ©è†œï¼šå›ºå®šåœ¨ GPU&float32ï¼›æ¯ä¸ª tile å–åˆ‡ç‰‡å¹¿æ’­åˆ°æ—¶é—´è½´
            Mj = Mj.to(device=dev, dtype=torch.float32, non_blocking=True)
    
            tcount = 0
            for x0 in range(0, Nx, tile_x):
                xs = slice(x0, min(x0 + tile_x, Nx))
    
                # ===== å–å‡ºä¸€ä¸ª tileï¼Œå¹¶å‡ç²¾åº¦ =====
                A_blk = self.A[xs, :, :].to(torch.complex128)      # (tx,Ny,Nt)
    
                # ===== (1) é¢‘åŸŸé‡Œæ„é€  NÌ‚(Ï‰) = ğ“•_t{|A|^2}ï¼ˆå« Ramanï¼‰=====
                I_hat = self.fft_t(torch.abs(A_blk)**2)[1]         # (tx,Ny,Nt)
                if H_R is not None:
                    # ((1-fR) + fR H_R) * I_hat â€”â€” åŸåœ°å†™æ³•ï¼Œçœä¸€æ¬¡ä¸´æ—¶
                    I_hat *= (1.0 - self.f_R)
                    I_hat += (self.f_R * H_R) * I_hat / (1.0 - self.f_R)
    
                # ===== (2) SPMï¼šÏ†(t) = ğ“•â»Â¹{Î³_even Â· NÌ‚}ï¼ˆå®æ•°ï¼‰ï¼Œåªåœ¨è†œå†…ç”Ÿæ•ˆ =====
                phi_t = self.ifft_t(I_hat * gamma_even)[1].real     # (tx,Ny,Nt) float64
                M_blk = Mj[xs, :].to(dtype=torch.float32)           # (tx,Ny)
                phase = (phi_t * dz) * M_blk[..., None]             # (tx,Ny,Nt) float32/64
                A_blk *= torch.exp(1j * phase)                      # çº¯ç›¸ä½ï¼ˆè†œå¤– phase=0ï¼‰
                del phi_t, phase
    
                # ===== (3) Shockï¼šdA_shock = (Î³0/Ï‰0) dz âˆ‚t(A N)ï¼Œå†ä¹˜è†œæ©è†œ =====
                N_t = self.ifft_t(I_hat)[1].real                    # (tx,Ny,Nt) float64
                del I_hat
    
                F_AN = self.fft_t(A_blk * N_t)[1]                   # (tx,Ny,Nt) complex128
                F_AN *= k_shock                                     # ä¹˜ (Î³0/Ï‰0)Â·dz
                dA_shock = self.ifft_t(1j * omega128 * F_AN)[1]     # (tx,Ny,Nt)
                del F_AN, N_t
    
                A_blk -= dA_shock * M_blk[..., None]                # åªåœ¨è†œå†…æ‰£é™¤ shock
                del dA_shock, M_blk
    
                # ===== (4) å›å†™ï¼ˆé™å› complex64ï¼‰ =====
                self.A[xs, :, :] = A_blk.to(torch.complex64)
                del A_blk
    
                tcount += 1
                if empty_cache_every and (tcount % empty_cache_every == 0):
                    torch.cuda.empty_cache()
                    
    @torch.inference_mode()
    def _apply_global_time_shift(self, dt: float) -> None:
        """å…¨å±€æ—¶é—´å¹³ç§»ï¼šA(t) â† A(t+dt)ï¼›é¢‘åŸŸä¹˜ e^{+iÎ© dt}ã€‚"""
        _, A_w = self.fft_t(self.A)
        self.A=None
        torch.cuda.empty_cache()
        phase = torch.exp(1j * self.omega.to(A_w.dtype) * float(dt))
        A_w *= phase[None, None, :]
        del phase
        torch.cuda.empty_cache()
        _, self.A = self.ifft_t(A_w)
    
    def _build_exclusive_region_masks(self) -> list[torch.Tensor]:
        """
        æŠŠ stepped_film ç´¯ç§¯æ©è†œå˜æˆäº’æ–¥åˆ†åŒºï¼š
          Region0=ä¸­å¿ƒåŒºï¼ŒRegionK-1=æœ€å¤–åŒºï¼Œä¸­é—´ä¸ºå„ç¯å¸¦ã€‚
        è¿”å› float32, device=self.T.device çš„åˆ—è¡¨ã€‚
        """
        assert self.stepped_film is not None and len(self.stepped_film.layers) > 0
        dev = self.T.device
        cum = [L.mask.to(device=dev, dtype=torch.float32, non_blocking=True) for L in self.stepped_film.layers]
        K = len(cum)
        excl = []
        if K == 1:
            excl = [cum[0]]
        else:
            for j in range(K - 1):
                excl.append(torch.clamp(cum[j] - cum[j + 1], 0.0, 1.0))
            excl.append(torch.clamp(cum[-1], 0.0, 1.0))
        return excl
    
    def _overlap_fraction(self, A: torch.Tensor, B: torch.Tensor) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªè½¯æ©è†œçš„é‡å å æ¯”ï¼šâŸ¨AÂ·BâŸ©/âŸ¨AâŸ©ã€‚A è§†ä¸ºâ€œåˆ†åŒºâ€ï¼ŒB è§†ä¸ºâ€œæœ¬å±‚â€ã€‚
        """
        num = (A * B).sum().item()
        den = A.sum().item()
        return 0.0 if den == 0.0 else (num / den)
                        
    @torch.inference_mode()
    def propagate_with_film(self, *,
                            apply_surfaces: bool = True,
                            store_steps: tuple[int, ...] = (),
                            callback: Optional[Callable[[int, "PulseGNLSESimulator"], None]] = None,
                            tile_x_kerr: int = 32,
                            eps_inc: float = 0.0,
                            skip_empty_masks: bool = True,
                            center_camera_midpoint: bool = False):   # â† æ–°å¢å‚æ•°
        """
        ï¼ˆâ€¦åŸæ–‡ä¸å˜â€¦ï¼‰
        """
        assert self.stepped_film is not None, "stepped_film æœªè®¾ç½®ï¼›å…ˆ set_stepped_film_from_annuli(...)"
    
        dev = self.T.device
        self.A = (self.A_in if self.A is None else self.A).to(dev, non_blocking=True)
    
        if apply_surfaces:
            self.A.mul_(torch.sqrt(torch.tensor(self.T_surf, device=dev, dtype=self.A.dtype)))
    
        snapshots: list[torch.Tensor] = []
        global_step = 0
    
        # â€”â€” æ‘„åƒæœºä¸­ç‚¹ï¼šé¢„å¤‡ â€”â€” #
        if center_camera_midpoint:
            regions = self._build_exclusive_region_masks()
            M_center = regions[0]
            M_outer  = regions[-1]
            # Î”Î²1 = Î²1_film - Î²1_ref
            delta_beta1 = float(self.materials["beta_film"][1] - self.materials["beta_ref"][1])  # [s/m]
    
        for j, layer in enumerate(self.stepped_film.layers):
            dL = float(layer.dL); nz = int(layer.nz)
            if nz <= 0 or dL == 0.0: continue
    
            dz = dL / nz
            dz_half = 0.5 * dz
    
            Mj = layer.mask.to(device=dev, dtype=torch.float32, non_blocking=True)
            has_film = (Mj.max().item() > 0.0)
    
            for s in range(nz):
                print(global_step)
                # ---- Â½ å‚è€ƒ(space) ----
                self._space_ref_half(dz_half)
                # ---- Â½ å‚è€ƒ(time) ----
                self._time_ref_half(dz_half)
    
                # ---- Â½ å¢é‡çº¿æ€§ï¼ˆè†œåŒºï¼‰----
                if has_film:
                    self._apply_incremental_linear_half(Mj, dz_half, eps=eps_inc)
    
                # ---- éçº¿æ€§ï¼ˆè†œåŒºï¼‰----
                if has_film:
                    self._kerr_step_with_film(dz, Mj, tile_x=tile_x_kerr)
    
                # ---- Â½ å¢é‡çº¿æ€§ï¼ˆè†œåŒºï¼‰----
                if has_film:
                    self._apply_incremental_linear_half(Mj, dz_half, eps=eps_inc)
    
                # ---- Â½ å‚è€ƒ(time) ----
                self._time_ref_half(dz_half)
                # ---- Â½ å‚è€ƒ(space) ----
                self._space_ref_half(dz_half)
    
                # === æ‘„åƒæœºå¯¹ä¸­ï¼šæŠŠâ€œä¸­å¿ƒåŒº & æœ€å¤–åŒºâ€çš„æœ¬æ­¥ GD å–å¹³å‡å¹¶åå‘å¹³ç§» ===
                if center_camera_midpoint and has_film:
                    # é‡å å æ¯”ï¼ˆè½¯è¾¹ç¼˜ï¼‰ï¼šå½“å‰å±‚æ˜¯å¦ä½œç”¨åˆ°ä¸­å¿ƒ/å¤–åŒº
                    fc = self._overlap_fraction(M_center, Mj)   # âˆˆ[0,1]
                    fo = self._overlap_fraction(M_outer,  Mj)
                    # æœ¬æ­¥ä¸¤ç«¯çš„å¢é‡ç¾¤æ—¶å»¶ï¼ˆç›¸å¯¹çœŸç©ºï¼‰
                    tau_c = delta_beta1 * dz * fc               # [s]
                    tau_o = delta_beta1 * dz * fo               # [s]
                    dt_shift = 0.5 * (tau_c + tau_o)            # [s] â€”â€” ä¸¤ç«¯ä¸­ç‚¹
                    if dt_shift != 0.0:
                        # åå‘åŠ å…¥åˆ°â€œç›¸æœºæ—¶é—´åŸç‚¹â€ä¸Šï¼šA(t)â†A(t+dt_shift)
                        self._apply_global_time_shift(dt_shift)
    
                # ---- å¿«ç…§ / å›è°ƒ ----
                if global_step in store_steps:
                    snapshots.append(self.A.detach().cpu().clone())
                if callback is not None:
                    callback(global_step, self)
                global_step += 1
    
        if apply_surfaces:
            self.A.mul_(torch.sqrt(torch.tensor(self.T_surf, device=dev, dtype=self.A.dtype)))
    
        self.A_out = self.A.detach().cpu()
        del self.A
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
        return snapshots
    

        
    @torch.inference_mode()
    def propagate_in_vacuum(self, L: float, nz: int,
                            *,
                            apply_surfaces: bool = False,
                            store_steps: tuple[int, ...] = (),
                            callback: Optional[Callable[[int, "PulseGNLSESimulator"], None]] = None):
        """
        ä»…åœ¨å‚è€ƒä»‹è´¨=çœŸç©ºä¸­ä¼ æ’­ Lï¼ˆç±³ï¼‰ï¼Œåˆ† nz æ­¥ã€‚
        ä¸ä½¿ç”¨è–„è†œä¸éçº¿æ€§ï¼›æ¯æ­¥åšï¼šÂ½L_ref(space) â†’ Â½L_ref(time) â†’ Â½L_ref(time) â†’ Â½L_ref(space)
        æ³¨æ„ï¼šè‹¥åˆå§‹åŒ–æ—¶ retarded_frame=Trueï¼Œåˆ™ Î²1_ref ä¼šè¢«å»æ‰ï¼Œç›¸å½“äºå…±åŠ¨å¸§ã€‚
        """
        dev = self.T.device
        
        if self.A is None:
            self.A = self.A_out.to(dev, non_blocking=True)
        else:
            self.A = self.A.to(dev, non_blocking=True)
    
        if apply_surfaces:
            self.A.mul_(torch.sqrt(torch.tensor(self.T_surf, device=dev, dtype=self.A.dtype)))
    
        dz = float(L) / int(nz)
        dz_half = 0.5 * dz
    
        # å±€éƒ¨æ„é€ çœŸç©ºæ—¶é—´åŠæ­¥ç®—å­ï¼Œé¿å… retarded_frame=True æ—¶è¢«è·³è¿‡
        Om = self.omega.to(torch.complex64)                   # (Nt,)
        Lw_half_vac = torch.exp(-1j * (1.0/self.c) * Om * dz_half)  # exp{-i Î²1_ref Î© dz/2}
    
        snapshots: list[torch.Tensor] = []
        flag=0
        for step in range(int(nz)):
            print(flag)
            flag+=1
            # Â½ space (vac)
            _, _, A_k = self.fft_xy(self.A)
            self.A = None
            torch.cuda.empty_cache()
            A_k *= torch.exp(self._coef_xy_ref * dz_half)[..., None]
            _, _, self.A = self.ifft_xy(A_k)
            del A_k
            torch.cuda.empty_cache()
            '''
            # Â½ time (vac)
            _, A_w = self.fft_t(self.A)
            self.A = None
            torch.cuda.empty_cache()
            A_w *= Lw_half_vac[None, None, :]
            _, self.A = self.ifft_t(A_w)
            del A_w
            torch.cuda.empty_cache()
            
            # Â½ time (vac)
            _, A_w = self.fft_t(self.A)
            self.A = None
            torch.cuda.empty_cache()
            A_w *= Lw_half_vac[None, None, :]
            _, self.A = self.ifft_t(A_w)
            del A_w
            torch.cuda.empty_cache()
            '''
            # Â½ space (vac)
            _, _, A_k = self.fft_xy(self.A)
            self.A = None
            torch.cuda.empty_cache()
            A_k *= torch.exp(self._coef_xy_ref * dz_half)[..., None]
            _, _, self.A = self.ifft_xy(A_k)
            del A_k
            torch.cuda.empty_cache()
    
            if step in store_steps:
                snapshots.append(self.A.detach().cpu().clone())
            if callback is not None:
                callback(step, self)
    
        if apply_surfaces:
            self.A.mul_(torch.sqrt(torch.tensor(self.T_surf, device=dev, dtype=self.A.dtype)))
    
        self.A_out = self.A.detach().cpu()
        del self.A
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
        return snapshots


    @torch.inference_mode()
    def _linear_half_step(self):
        with torch.no_grad():
             # 1) (x,y,t) â†’ (kx,ky,t)
            _, _, A = self.fft_xy(self.A)              # A: (Nx,Ny,Nt), complex64
            self.A = None
            torch.cuda.empty_cache()
            # 2) (kx,ky,t) â†’ (kx,ky,Ï‰)
            _, A = self.fft_t(A)                       # A: (Nx,Ny,Nt)
                
            # 3) ä¹˜ä»¥åˆ†ç¦»çš„çº¿æ€§åŠæ­¥å› å­ï¼ˆå¹¿æ’­ï¼Œä¸ä¼šå¤åˆ¶å¤§å¼ é‡ï¼‰
            A *= self.Lxy_half[..., None]              # (Nx,Ny,1) å¹¿æ’­åˆ° (Nx,Ny,Nt)
            A *= self.Lw_half[None, None, :]           # (1,1,Nt) å¹¿æ’­åˆ° (Nx,Ny,Nt)
                
            # 4) (kx,ky,Ï‰) â†’ (kx,ky,t) â†’ (x,y,t)
            _, A = self.ifft_t(A)
            _, _, A = self.ifft_xy(A)
                
            self.A = A                                 # å›å†™

    
    @torch.inference_mode()    
    def _kerr_step(self, dz: float, *, tile_x: int = 32, empty_cache_every: int = 0):
        """
        å†…å­˜å‹å¥½çš„ Kerr æ­¥ï¼šæŒ‰ x æ–¹å‘åˆ†å— (tile_x Ã— Ny Ã— Nt) æµå¼å¤„ç†ã€‚
        - ç‰©ç†ä¸å˜ï¼šSPM ä»ä¸ºçº¯ç›¸ä½ï¼›shock ä»ä¸º -(Î³0/Ï‰0) âˆ‚t(A N)ã€‚
        - æ•°å€¼ï¼šä»…åœ¨æœ¬å‡½æ•°å†…éƒ¨ä¸´æ—¶å‡åˆ° complex128ï¼›æ¯ä¸ª tile å°±åœ°æ›´æ–°å›å†™ self.Aï¼ˆcomplex64ï¼‰ã€‚
        - å‚æ•°ï¼š
            tile_x:   æ¯æ¬¡å¤„ç†çš„ x æ¡æ•°ï¼ˆ32/64 ç»éªŒä¸Šè¾ƒå¥½ï¼‰
            empty_cache_every: æ¯å¤„ç†å¤šå°‘ä¸ª tile è°ƒä¸€æ¬¡ empty_cacheï¼›0 è¡¨ç¤ºä¸è°ƒ
        """
        with torch.no_grad():
            Nx, Ny, Nt = self.Nx, self.Ny, self.nt
            k_shock = (self.gamma0 / self.omega0) * dz
    
            gamma_even = self.gamma_w_even128                         # (1,1,Nt)
            omega128   = self.omega128                                # (Nt,)
            H_R = self.H_R_omega128 if self.raman_on else None
    
            # ä¸»å¾ªç¯ï¼šæŒ‰ x æ–¹å‘åˆ‡ç‰‡
            tcount = 0
            for x0 in range(0, Nx, tile_x):
                xs = slice(x0, min(x0 + tile_x, Nx))
    
                # ===== (0) å–å‡ºä¸€ä¸ª tileï¼Œå¹¶å‡ç²¾åº¦åˆ° complex128 =====
                A_blk = self.A[xs, :, :].to(torch.complex128)         # (tx,Ny,Nt)
    
                # ===== (1) NÌ‚(Ï‰) = ğ“•_t{|A|Â²}ï¼Œä¹˜ä¸Š Raman å“åº” =====
                I_hat = self.fft_t(torch.abs(A_blk)**2)[1]            # (tx,Ny,Nt)
                if H_R is not None:
                    # ((1-fR) + fR H_R) * I_hat  â€”â€” å°±åœ°ä¹˜æ³•å‡å°‘æ–°å¼ é‡
                    I_hat *= (1.0 - self.f_R)
                    I_hat += (self.f_R * H_R) * I_hat / (1.0 - self.f_R)  # å¤ç”¨ç¼“å†²
    
                # ===== (2) SPMï¼šphi(t) = ğ“•â»Â¹{ Î³_even Â· NÌ‚ } â†’ çº¯ç›¸ä½ =====
                phi_t = self.ifft_t(I_hat * gamma_even)[1].real        # (tx,Ny,Nt)
                # å°±åœ°ç›¸ä½æ›´æ–°ï¼šA_blk *= exp(i*phi*dz)
                A_blk *= torch.exp(1j * phi_t * dz)
                # ç«‹åˆ»é‡Šæ”¾ phi_tï¼Œé™ä½å³°å€¼æ˜¾å­˜
                del phi_t
                
                # ===== (3) è‡ªé™¡å³­ï¼šN(t) = ğ“•â»Â¹{NÌ‚}ï¼›k_shock å…ˆä¹˜åˆ°é¢‘åŸŸ =====
                N_t = self.ifft_t(I_hat)[1].real                       # (tx,Ny,Nt)
                del I_hat                                              # NÌ‚ ä¸å†éœ€è¦
    
                F_AN = self.fft_t(A_blk * N_t)[1]                      # (tx,Ny,Nt)
                # å…ˆç¼©æ”¾ï¼Œé¿å…ä¸­é—´é‡æ”¾å¤§æº¢å‡º
                F_AN *= k_shock
                dA_shock = self.ifft_t(1j * omega128 * F_AN)[1]        # (tx,Ny,Nt)
                del F_AN, N_t
    
                A_blk -= dA_shock
                del dA_shock
                
                
    
                # ===== (4) å›å†™ï¼ˆé™å› complex64ï¼‰ï¼Œé‡Šæ”¾ tile å†…å­˜ =====
                self.A[xs, :, :] = A_blk.to(torch.complex64)
                del A_blk
    
                tcount += 1
                if empty_cache_every and (tcount % empty_cache_every == 0):
                    torch.cuda.empty_cache()

        
    

    # --------------------------------------------------------------
    #  åœ¨ PulseGNLSESimulator å†…éƒ¨
    # --------------------------------------------------------------
    @torch.inference_mode()    
    def propagate(
            self,
            n_steps: int | None = None,
            store_steps: tuple[int, ...] = (),
            callback: Optional[Callable[[int, "PulseGNLSESimulator"], None]] = None,
            report_every: int = 1,
    ):
        """
        Strang-split propagation: Â½Linear â†’ Kerr â†’ Â½Linear, with snapshot support.
    
        Parameters
        ----------
        n_steps : int | None
            ä¼ æ’­æ­¥æ•°ï¼›é»˜è®¤ = self.nz
        store_steps : tuple[int]
            éœ€è¦ä¿å­˜å¿«ç…§çš„æ­¥å·ï¼ˆ0-basedï¼‰ã€‚å¯ä¼  range(...)
        callback : callable(step_idx:int, self), optional
            æ¯æ­¥ç»“æŸåæ‰§è¡Œï¼Œä¾‹å¦‚ tqdm è¿›åº¦æ¡æˆ–è¯Šæ–­

        Returns
        -------
        list[np.ndarray]
            ä¿å­˜çš„ A(x,y,t) å‰¯æœ¬ï¼Œé¡ºåºä¸ store_steps ä¸€è‡´
        """
        with torch.no_grad():
            self.A = self.A.to(DEVICE)     # ä¼ æ’­å‰æ¬å› GPU
            self.A = self.A * torch.sqrt(torch.tensor(self.T_surf, device=DEVICE))
    
            
            if n_steps is None:
                n_steps = self.nz
            dz = self.dz
            snapshots: list[torch.Tensor] = []                        # â† å­˜å¿«ç…§
            
            for step in range(n_steps):
                print(step)               
        
                # -------- Â½ çº¿æ€§ --------
                self._linear_half_step()
                torch.isnan(self.A).any() and print('NaN after Â½lin', step)
                
                # -------- Kerr éçº¿æ€§ --------
                self._kerr_step(dz)
                #print('Kerr')
                torch.isnan(self.A).any() and print('NaN after kerr', step)

                # -------- Â½ çº¿æ€§ --------
                self._linear_half_step()
                #print('2nd linear')
                torch.isnan(self.A).any() and print('NaN after Â½lin', step)

                # -------- è®°å½• / å›è°ƒ --------
                diag = self.diagnose(domain='space', metrics=('fwhm',))
                self.spatial_w.append(diag['metrics']['fwhm']['x'])
                del diag
                torch.cuda.empty_cache()
                diag2 = self.diagnose(domain='kspace', metrics=('fwhm',))
                self.spatial_freq_w.append(diag2['metrics']['fwhm']['kx'])
                del diag2
                torch.cuda.empty_cache()
                #print('record')
                
                if step in store_steps:
                    snapshots.append(self.A.clone())      # æ·±æ‹·è´
                if callback is not None:
                    callback(step, self)
                
    
    
            self.A = self.A * torch.sqrt(torch.tensor(self.T_surf, device=DEVICE))       
            beta2_mgf2 = 2.05e-26             # s^2/m
            L_win      = 5.1e-3               # m
            phase_win  = 0.5 * beta2_mgf2 * (self.omega**2) * L_win   # å› ä¸º factorial(2)=2
            _, A_w = self.fft_t(self.A)
            #_, self.A = self.ifft_t(A_w * torch.exp(1j*phase_win))
            self.A_out = self.A.detach().cpu()
            del self.A
            return snapshots
                
    @torch.inference_mode()
    def apply_circular_lowpass_by_factor(
        self,
        factor: float,
        *,
        A: torch.Tensor | np.ndarray | None = None,
        field: str = "output",
        dk_fwhm: float | None = None,
        dk_rel: float = 0.0,
        tile_t: int = 32,
        preserve_energy: bool = False,
        to_cpu: bool = True,
    
        # â€”â€” é¢„è§ˆå‚æ•° â€”â€”
        preview: bool = True,
        preview_shift: bool = True,
        preview_logscale: bool = True,
        preview_db_floor: float = -60.0,
        preview_percentile: float = 99.9,
        preview_span_factor: float = 1.5
    ) -> torch.Tensor:
        """
        åŸºäºâ€œåˆå§‹ A_in çš„ç©ºé—´é¢‘è°± FWHMâ€çš„åœ†å½¢ä½é€šï¼š
            k_cut = factor * (self.k_fwhm0 / 2)
        é¢„è§ˆä»…ç”¨äºæ˜¾ç¤ºï¼›æ»¤æ³¢ç»“æœå†™ self.A_outï¼ˆCPUï¼‰å¹¶è¿”å›ã€‚
        """
        import numpy as np
        import matplotlib.pyplot as plt
        from matplotlib.patches import Circle
        from torch import fft as tfft
    
        # â€”â€” å…œåº•ï¼šæ²¡æœ‰ k_fwhm0 å°±ä¸´æ—¶æµ‹ä¸€æ¬¡ â€”â€”
        if not hasattr(self, "k_fwhm0") or not np.isfinite(getattr(self, "k_fwhm0", np.nan)):
            A_saved = getattr(self, "A", None)
            try:
                Ain_gpu = (self.A_in if torch.is_tensor(self.A_in) else torch.as_tensor(self.A_in)).to(self.T.device)
                self.A  = Ain_gpu
                diag_k  = self.diagnose(domain='kspace', metrics=('fwhm',))
                self.kx_fwhm0 = float(diag_k['metrics']['fwhm']['kx'])
                self.ky_fwhm0 = float(diag_k['metrics']['fwhm']['ky'])
                self.k_fwhm0  = 0.5 * (self.kx_fwhm0 + self.ky_fwhm0)
            finally:
                self.A = A_saved
                if torch.cuda.is_available(): torch.cuda.empty_cache()
    
        # â€”â€” é€‰æ‹©è¾“å…¥åœº â€”â€”
        if A is None:
            if field == "output":
                if self.A_out is None:
                    raise ValueError("apply_circular_lowpass_by_factor: self.A_out is Noneï¼›å…ˆå¾—åˆ°è¾“å‡ºåœºæˆ–æ˜¾å¼ä¼ å…¥ Aã€‚")
                A_src = self.A_out
            elif field == "current":
                if self.A is None:
                    raise ValueError("apply_circular_lowpass_by_factor: self.A is Noneï¼›ä¼ æ’­è¿‡ç¨‹ä¸­æ‰æœ‰ current åœºã€‚")
                A_src = self.A
            elif field == "input":
                if self.A_in is None:
                    raise ValueError("apply_circular_lowpass_by_factor: self.A_in is Noneã€‚")
                A_src = self.A_in
            else:
                raise ValueError("field å¿…é¡»æ˜¯ 'output' | 'current' | 'input'")
        else:
            A_src = A
    
        A_src = (A_src if torch.is_tensor(A_src) else torch.as_tensor(A_src))
        if A_src.shape != (self.Nx, self.Ny, self.nt):
            raise ValueError(f"è¾“å…¥åœºå½¢çŠ¶ {tuple(A_src.shape)} â‰  {(self.Nx,self.Ny,self.nt)}")
    
        dev = self.T.device
        A_src = A_src.to(device=dev, dtype=torch.complex64, non_blocking=True)
    
        # â€”â€” æˆªæ­¢åŠå¾„ & è½¯è¾¹ â€”â€”
        k_cut = float(factor) * (float(self.k_fwhm0) * 0.5)
        if dk_fwhm is None:
            dk_fwhm = float(dk_rel) * max(k_cut, 1e-12)
        dk_fwhm = float(dk_fwhm)
    
        # ä½é€šæ ¸
        K_perp = torch.sqrt(self.K_perp2).to(dev)
        Hk     = soft_unit_step(k_cut - K_perp, dk_fwhm).to(torch.float32)  # (Nx,Ny)âˆˆ[0,1]
    
        # â€”â€” åˆ†å—æ²¿ t å¤„ç† â€”â€”
        Nt = int(self.nt)
        dx, dy, dT    = float(self.dx), float(self.dy), float(self.dT)
        scale_fft_xy  = float(self._scale_fft_xy.detach().cpu())
        scale_ifft_xy = float(self._scale_ifft_xy.detach().cpu())
    
        out = torch.empty_like(A_src, device=dev)
        E_in = 0.0
        E_ot = 0.0
    
        # é¢„è§ˆç´¯è®¡ï¼šæ»¤æ³¢å‰çš„ I(kx,ky) = âˆ« |Ã‚|Â² dt ï¼ˆ**æ¢¯å½¢ç§¯åˆ†**ï¼‰
        I_accum = torch.zeros((self.Nx, self.Ny), device=dev, dtype=torch.float32) if preview else None
    
        for t0 in range(0, Nt, int(tile_t)):
            t1   = min(t0 + int(tile_t), Nt)
            Ablk = A_src[:, :, t0:t1]
    
            # å…¥èƒ½ï¼ˆå—ï¼‰
            Pin = torch.sum(torch.abs(Ablk)**2, dim=(0,1)) * dx * dy
            E_in += float(torch.sum(Pin).item() * dT)
    
            # 2D FFT
            Ak = tfft.fftn(Ablk, dim=(0, 1))
            Ak.mul_(scale_fft_xy)
    
            # â€”â€” é¢„è§ˆç§¯åˆ†ï¼šæ¢¯å½¢æƒé‡ï¼ˆé¦–æœ«ç«¯ 0.5ï¼‰â€”â€”
            if I_accum is not None:
                pow2 = Ak.real*Ak.real + Ak.imag*Ak.imag                  # (Nx,Ny,dt)
                w = torch.ones((t1 - t0), device=dev, dtype=pow2.dtype)
                if t0 == 0:  w[0]  = 0.5
                if t1 == Nt: w[-1] = 0.5
                I_accum.add_(torch.sum(pow2 * w[None, None, :], dim=2) * dT)
    
            # æ»¤æ³¢ â†’ iFFT
            Ak *= Hk[..., None].to(Ak.dtype)
            Ablk = tfft.ifftn(Ak, dim=(0, 1))
            Ablk.mul_(scale_ifft_xy)
            del Ak
    
            out[:, :, t0:t1] = Ablk
    
            # å‡ºèƒ½ï¼ˆå—ï¼‰
            Pout = torch.sum(torch.abs(Ablk)**2, dim=(0,1)) * dx * dy
            E_ot += float(torch.sum(Pout).item() * dT)
            del Ablk
    
        # å¯é€‰èƒ½é‡ä¿æŒ
        if preserve_energy and E_ot > 0.0:
            out.mul_(np.sqrt(E_in / E_ot))
    
        # â€”â€” é¢„è§ˆç»˜å›¾ï¼ˆåæ ‡/shift/çº¿è°±ä¸ visualize_simulator å®Œå…¨ä¸€è‡´ï¼‰â€”â€”
        if I_accum is not None:
            with torch.no_grad():
                # é€šè¿‡èƒ½é‡å æ¯”ï¼ˆåŸºäºæœª shift çš„æ ¸ï¼‰
                num = float(torch.sum(I_accum * Hk).item())
                den = float(torch.sum(I_accum).item())
                pass_frac = (num / den) if den > 0 else 0.0
    
                # åŸç”Ÿåæ ‡
                I2d = I_accum.detach().cpu().numpy()        # (Nx,Ny)
                Kx1 = self.Kx[:, 0].detach().cpu().numpy()  # (Nx,)
                Ky1 = self.Ky[0, :].detach().cpu().numpy()  # (Ny,)
    
                # ä»…æ˜¾ç¤ºç”¨çš„ shift
                if preview_shift:
                    I2d_show = np.fft.fftshift(I2d, axes=(0,1))
                    Kx_show  = np.fft.fftshift(Kx1)
                    Ky_show  = np.fft.fftshift(Ky1)
                else:
                    I2d_show = I2d
                    Kx_show  = Kx1
                    Ky_show  = Ky1
    
                # åŠ¨æ€èŒƒå›´
                pval = np.percentile(I2d_show, preview_percentile)
                pval = pval if pval > 0 else (I2d_show.max() if I2d_show.max()>0 else 1.0)
                if preview_logscale:
                    img = 10.0 * np.log10(np.maximum(I2d_show / pval, 10.0**(preview_db_floor/10.0)))
                    vmin, vmax = preview_db_floor, 0.0
                    cbar_label = f"dB (ref {preview_percentile:.1f}%-ile)"
                else:
                    img = np.clip(I2d_show, 0.0, pval)
                    vmin, vmax = 0.0, pval
                    cbar_label = "âˆ«|Ã‚|Â² dt (arb)"
    
                # è§†é‡
                span = max(1e-6, float(preview_span_factor) * max(k_cut, 1e-12))
    
                # ç”»å›¾
                fig, (ax2d, ax1d) = plt.subplots(1, 2, figsize=(11, 4.5))
    
                # 2D é¢‘è°±
                extent = [Kx_show.min(), Kx_show.max(), Ky_show.min(), Ky_show.max()]
                im = ax2d.imshow(img, origin="lower", extent=extent, aspect="equal",
                                 vmin=vmin, vmax=vmax)
                fig.colorbar(im, ax=ax2d, label=cbar_label)
                ax2d.set_xlabel("kâ‚“ [mâ»Â¹]"); ax2d.set_ylabel("k_y [mâ»Â¹]")
                ax2d.set_title(f"Pre-filter spectrum  (predicted pass ~ {pass_frac*100:.1f}%)")
                ax2d.set_xlim(-span, +span); ax2d.set_ylim(-span, +span)
                ax2d.add_patch(Circle((0.0, 0.0), radius=k_cut, fill=False, lw=1.5))
    
                # 1D çº¿è°±ï¼škyâ‰ˆ0 åˆ‡ç‰‡ï¼ˆä¸ visualize_simulator ç›¸åŒï¼‰
                idx_ky0  = int(np.argmin(np.abs(Ky_show)))
                Ikx_line = I2d_show[:, idx_ky0]
                ax1d.plot(Kx_show, Ikx_line, label="|Ã‚|Â² at kyâ‰ˆ0")
                ax1d.axvline(+k_cut, ls="--", lw=1.0, label="Â± k_cut")
                ax1d.axvline(-k_cut, ls="--", lw=1.0)
                ax1d.set_xlim(-span, +span)
                ax1d.set_xlabel("kâ‚“ [mâ»Â¹]"); ax1d.set_ylabel("Power [arb]")
                ax1d.set_title("Line spectrum (kyâ‰ˆ0)")
                ax1d.grid(True); ax1d.legend()
    
                fig.tight_layout(); plt.show()
    
                if pass_frac < 1e-3:
                    print("[WARN] è¯¥å‚æ•°ä¸‹é¢„è®¡é€šè¿‡èƒ½é‡ < 0.1%ã€‚è¯·æ£€æŸ¥ k_fwhm0 æ˜¯å¦åˆç†ï¼Œæˆ–è°ƒå° factor / æ”¾å®½ dkã€‚")
    
        # â€”â€” å†™å› CPU â€”â€”
        A_cpu = out.detach().cpu()
        if to_cpu:
            self.A_out = A_cpu
        del out, A_src
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        return A_cpu


    
    # ===========================================================
    #   Diagnostics:  fwhm / rms   in space / time / kspace / freq
    # ===========================================================
    @torch.inference_mode()
    def diagnose(self, A=None, *, domain='space',
                 metrics=('fwhm', 'rms'), save_raw=False,
                 align_time: str = 'none',   # 'none' | 'peak' | 'centroid'
                 subsample_align: bool = True,  # ç”¨é¢‘åŸŸç›¸ç§»å®ç°äºšé‡‡æ ·å¯¹é½
                 report_center: bool = True):   # æŠ¥å‘Š t_peak / t_centroid

        """
        Quick diagnostic on the given 3-D envelope A(x,y,t).

        Parameters
        ----------
        A : ndarray | None
            If None, use self.A (current field).
        domain : str
            'space', 'time', 'kspace', or 'freq'.
        metrics : iterable[str]
            Any subset of {'fwhm','rms'}.
        save_raw : bool
            If True, return the 1-D projection curves.

        Returns
        -------
        dict
        """
        flag=1
        if A is None:
            A = self.A
            flag+=1
        if torch.is_tensor(A):
            A = A.detach().cpu().numpy()
            flag+=1

        def to_np(x):
            return x.detach().cpu().numpy() if torch.is_tensor(x) else np.asarray(x)
        x     = to_np(self.x)
        y     = to_np(self.y)
        T     = to_np(self.T)
        Kx    = to_np(self.Kx)
        Ky    = to_np(self.Ky)
        omega = to_np(self.omega)

        metrics = set(metrics)
        out = {'domain': domain, 'metrics': {}}

        # ---------- build 1-D projections -----------------------
        if domain == 'space':
            I_xy = np.sum(np.abs(A)**2, axis=2) * self.dT
            proj = {'x': np.trapezoid(I_xy, y, axis=1),
                    'y': np.trapezoid(I_xy, x, axis=0)}
            grid = {'x': x, 'y': y}

        elif domain == 'time':
            # ---- åŸå§‹æŠ•å½± P(t) --------------------------------------
            P_t  = np.sum(np.abs(A)**2, axis=(0,1)) * self.dx * self.dy
            T    = to_np(self.T)
            dT   = float(T[1] - T[0])

            # ---- æ‰¾ä¸­å¿ƒï¼ˆå³°å€¼/è´¨å¿ƒï¼‰ --------------------------------
            t_peak     = float(T[np.argmax(P_t)]) if P_t.max() > 0 else 0.0
            t_centroid = self._temporal_centroid_np(T, P_t)

            # ---- å¯é€‰å¯¹é½ï¼ˆä¸æ”¹ self.Aï¼Œåªæ”¹è¯Šæ–­ç”¨çš„ 1D æ›²çº¿ï¼‰ -------
            P_plot = P_t.copy()
            T_plot = T.copy()
            if align_time in ('peak', 'centroid'):
                t0 = t_peak if align_time == 'peak' else t_centroid
                if subsample_align:
                    # é¢‘åŸŸç›¸ç§»ï¼ˆäºšé‡‡æ ·ï¼‰
                    P_plot = self._shift_1d_np(T, P_plot, shift=-t0, d=dT)
                else:
                    # æ•´æ•°é‡‡æ · rollï¼ˆå¿«ï¼Œä½†æœ‰ Â±0.5 æ ·æœ¬è¯¯å·®ï¼‰
                    n = int(np.rint(t0 / dT))
                    P_plot = np.roll(P_plot, -n)
                # åæ ‡ç³»å‚è€ƒä¹Ÿç§»åˆ°ä»¥ 0 ä¸ºä¸­å¿ƒï¼ˆæ›´ç›´è§‚ï¼‰
                T_plot = T - t0

            proj = {'t': P_plot}
            grid = {'t': T_plot}


        elif domain == 'kspace':
            if flag==3:
                _, _, A_k = self.fft_xy(self.A)
                # back to CPU/numpy for projection
                A_k = A_k.detach().cpu().numpy()
                Ik_xy = np.sum(np.abs(A_k)**2, axis=2) * self.dT
                proj = {'kx': np.trapezoid(Ik_xy, Ky[0, :], axis=1),  # integrate over ky (axis=1)
                        'ky': np.trapezoid(Ik_xy, Kx[:,0], axis=0)}   # integrate over kx (axis=0)
                grid = {'kx': Kx[:,0], 'ky': Ky[0,:]}
            else:
                A_gpu = torch.from_numpy(A).to(self.A.device)
                # GPU FFT
                _, _, A_k = self.fft_xy(A_gpu)
                # back to CPU/numpy for projection
                A_k = A_k.detach().cpu().numpy()
                Ak_abs2 = (A_k.real**2 + A_k.imag**2)                  # â†’ å®æ•°ã€éè´Ÿ
                Ik_xy    = Ak_abs2.sum(axis=2) * float(self.dT)         # âˆ«|Ã‚|^2 dtï¼ˆçŸ©å½¢æ±‚ç§¯ï¼‰

                proj = {'kx': np.trapezoid(Ik_xy, Ky[0, :], axis=1),  # integrate over ky (axis=1)
                        'ky': np.trapezoid(Ik_xy, Kx[:,0], axis=0)}   # integrate over kx (axis=0)
                grid = {'kx': Kx[:,0], 'ky': Ky[0,:]}


        elif domain == 'freq':
            if flag==3:
                _, A_w_gpu = self.fft_t(self.A)
                A_w = A_w_gpu.detach().cpu().numpy()
                S_w = np.sum(np.abs(A_w)**2, axis=(0,1)) * self.dx * self.dy
                proj = {'w': S_w}
                grid = {'w': omega}
            else:
                A_gpu = torch.from_numpy(A).to(self.A.device)
                _, A_w_gpu = self.fft_t(A_gpu)
                A_w = A_w_gpu.detach().cpu().numpy()
                S_w = np.sum(np.abs(A_w)**2, axis=(0,1)) * self.dx * self.dy
                proj = {'w': S_w}
                grid = {'w': omega}

        else:
            raise ValueError("domain must be 'space' | 'time' | 'kspace' | 'freq'")

        # ---------- metrics ------------------------------------
        # ---------- metrics ------------------------------------
        if 'rms' in metrics:
            out['metrics']['rms'] = {ax: self._rms_width(grid[ax], y)
                                     for ax, y in proj.items()}
        if 'fwhm' in metrics:
            out['metrics']['fwhm'] = {ax: self._fwhm(grid[ax], y)
                                      for ax, y in proj.items()}

        # ---------- optional raw data & centers -----------------
        if save_raw:
            out['raw']  = proj
            out['grid'] = grid

        if report_center and domain == 'time':
            out.setdefault('center', {})
            out['center']['t_peak']     = t_peak
            out['center']['t_centroid'] = t_centroid

        return out


    # ---------- helper functions ------------------------------
    @staticmethod
    def _fwhm(x, y):
        x = np.asarray(x)
        y = np.abs(np.asarray(y))
        if y.size == 0 or y.max() == 0:
            return 0.0
    
        half = 0.5 * y.max()
        mask = y >= half
        idx  = np.flatnonzero(mask)
        if idx.size < 2:
            return 0.0
    
        # æ˜¯å¦â€œè·¨ç«¯â€ï¼ˆä¸¤ç«¯éƒ½â‰¥halfï¼ŒFFT åŸç”Ÿé¡ºåºå¸¸è§ï¼‰
        wraps = mask[0] and mask[-1]
        if wraps:
            # å±•å¼€æˆä¸¤å€é•¿åº¦å†å–é¦–å°¾
            x_step = x[1] - x[0]
            x = np.concatenate([x, x + x[-1] + x_step])
            y = np.concatenate([y, y])
            mask = np.concatenate([mask, mask])
            idx  = np.flatnonzero(mask)
    
        i0, i1 = idx[0], idx[-1]
    
        # --- å·¦è¾¹ç•Œï¼šç”¨ [j0, j0+1] çº¿æ€§æ’å€¼ï¼Œj0 è‡³å°‘ 0
        j0 = max(i0 - 1, 0)
        if j0 + 1 >= len(x):
            x_l = x[j0]
        else:
            x_l = np.interp(half, y[j0:j0+2], x[j0:j0+2])
    
        # --- å³è¾¹ç•Œï¼šç”¨ [j1, j1+1] çº¿æ€§æ’å€¼ï¼Œj1 è‡³å¤š len(x)-2
        j1 = min(i1, len(x) - 2)
        if j1 + 1 >= len(x):
            x_r = x[j1]
        else:
            x_r = np.interp(half, y[j1:j1+2], x[j1:j1+2])
    
        return float(x_r - x_l)

    
        # çº¿æ€§æ’å€¼
        def _interp(i_left):
            return np.interp(half, y[i_left:i_left+2], x[i_left:i_left+2])
        x_l = _interp(i0-1)
        x_r = _interp(i1)
        return x_r - x_l


    @staticmethod
    def _rms_width(x, y):
        y = np.abs(y)
        norm = np.trapz(y, x)
        if norm == 0:
            return 0.0
        mean = np.trapz(x * y, x) / norm
        var  = np.trapz((x - mean)**2 * y, x) / norm
        return np.sqrt(var)

    @staticmethod
    def _temporal_centroid_np(T: np.ndarray, P: np.ndarray) -> float:
        """è´¨å¿ƒ t_cm = âˆ« t P(t) dt / âˆ« P(t) dtï¼›è‹¥èƒ½é‡ä¸º 0 åˆ™è¿”å› 0."""
        denom = np.trapz(P, T)
        if denom == 0:
            return 0.0
        return float(np.trapz(T * P, T) / denom)

    @staticmethod
    def _shift_1d_np(x: np.ndarray, y: np.ndarray, shift: float, *, d: float) -> np.ndarray:
        """
        æŠŠ y(x) åœ¨ x è½´ä¸Šå¹³ç§» 'shift'ï¼ˆå¯åˆ†æ•°é‡‡æ ·ï¼‰ï¼Œè¿”å›ç§»ä½åçš„ yã€‚
        ç”¨é¢‘åŸŸç›¸ä½å¡åº¦å®ç°ï¼šY(k) * exp(-i k shift)ã€‚
        """
        # é¢‘ç‡åæ ‡ï¼ˆä¸ np.fft.fftfreq ä¸€è‡´ï¼‰
        k = 2*np.pi*np.fft.fftfreq(y.size, d=d)
        Y = np.fft.fft(y)
        y_shift = np.fft.ifft(Y * np.exp(-1j * k * shift))
        # å¯¹äºåŠŸç‡æ›²çº¿ï¼Œè™šéƒ¨åº”ä¸ºæ•°å€¼å™ªå£°
        return np.real(y_shift)

from dataclasses import dataclass
from typing import List, Sequence

# -------------------------------
# Stepped-film data structures
# -------------------------------

@dataclass
class LayerSpec:
    """A single physical layer in the stepped film."""
    dL: float                   # layer thickness Î”L_j  [m]
    nz: int                     # axial steps within this layer
    mask: torch.Tensor          # M_j(x,y) in [0,1], shape (Nx,Ny), float32, device=sim.X.device
    name: str = ""              # (optional) tag for debugging/plots


class SteppedFilm:
    """Holds all layers and convenience accessors."""
    def __init__(self, layers: Sequence[LayerSpec]):
        self.layers: List[LayerSpec] = list(layers)

    @property
    def K(self) -> int:
        return len(self.layers)

    def thickness_map(self) -> torch.Tensor:
        """L(x,y) = sum_j Î”L_j * M_j(x,y)  (float32, same device as masks)."""
        assert self.layers, "No layers"
        dev = self.layers[0].mask.device
        Lxy = torch.zeros_like(self.layers[0].mask, dtype=torch.float32, device=dev)
        for L in self.layers:
            Lxy = Lxy + float(L.dL) * L.mask
        return Lxy


import re
import numpy as np
import pandas as pd

def _parse_r_range_mm_to_m(s: str) -> tuple[float, float]:
    """
    æŠŠ '0.4â€“0.7' / '0.4-0.7' / '0.4 â€” 0.7' è¿™æ ·çš„åŠå¾„åŒºé—´(å•ä½mm)è§£ææˆ (lo_m, hi_m)ã€‚
    """
    parts = re.split(r"\s*[-â€“â€”]\s*", str(s).strip())
    if len(parts) != 2:
        raise ValueError(f"Bad r range string: {s!r}")
    lo_mm = float(parts[0]); hi_mm = float(parts[1])
    return lo_mm * 1e-3, hi_mm * 1e-3  # â†’ m


def csv_to_layers_lists(
    csv_path: str,
    *,
    col_range: str = "r range (mm)",
    col_thickness: str = "t_final (mm)",   # ä¹Ÿå¯æ¢æˆ 't_flat (mm)'
    names_col: str | None = "Layer k",
    drop_const_out: bool = True,           # æœ«è¡Œ const(out) ä¸”å¢é‡â‰ˆ0 æ—¶ä¸¢å¼ƒ
    eps_nm: float = 1.0                    # å°äºè¯¥é˜ˆå€¼çš„åšåº¦å¢é‡è§†ä¸º0
) -> tuple[list[float | None], list[float], list[str]]:
    """
    ä» CSV ç”Ÿæˆ (inner_radii, dL_list, names)ï¼š
    - inner_radii[0] = None è¡¨ç¤ºç¬¬ä¸€å±‚å…¨ç‰‡ï¼›
    - dL_list[0] = t[0]ï¼›dL_list[j] = t[j] - t[j-1]ï¼ˆæŠŠæ¯å±‚åšæˆâ€œå°é˜¶å¢é‡â€ï¼‰ã€‚
    - å•ä½ï¼šè¿”å›å€¼å‡ä¸ºâ€œç±³â€ã€‚
    """
    df = pd.read_csv(csv_path)

    # 1) è§£ææ¯è¡ŒåŠå¾„åŒºé—´
    ranges_m = [ _parse_r_range_mm_to_m(s) for s in df[col_range].tolist() ]
    r_lo = [lo for lo, _ in ranges_m]
    r_hi = [hi for _, hi in ranges_m]

    # 2) åšåº¦ï¼ˆmï¼‰â€”â€” é»˜è®¤ä½¿ç”¨ t_final
    t_m = (df[col_thickness].astype(float).to_numpy()) * 1e-3

    # 3) å¢é‡åˆ—è¡¨ï¼šç¬¬ä¸€å±‚æ˜¯å…¨ç‰‡åšåº¦ï¼Œå…¶åæ˜¯ç›¸é‚»å±‚åšåº¦å·®
    dL = np.empty_like(t_m)
    dL[0] = t_m[0]
    dL[1:] = t_m[1:] - t_m[:-1]

    # 4) æ¸…ç†ï¼šæŠŠâ€œç»å¯¹å€¼ < eps_nmâ€è§†ä¸º 0ï¼›å¦‚å‡ºç°è´Ÿå¢é‡ï¼ŒæŒ‰ 0 å¤¹ç´§å¹¶æç¤º
    eps_m = eps_nm * 1e-9
    tiny = np.abs(dL) < eps_m
    dL[tiny] = 0.0
    if np.any(dL < -eps_m):
        print("[csv_to_layers_lists] WARNING: nonâ€‘monotonic thickness detected; "
              "negative increments were clipped to 0.")
        dL = np.maximum(dL, 0.0)

    # 5) inner_radiiï¼šç¬¬ä¸€å±‚ Noneï¼ˆå…¨ç‰‡ï¼‰ï¼Œä¹‹åç”¨æ¯è¡Œâ€œåŒºé—´ä¸‹ç•Œâ€
    inner_radii: list[float | None] = [None] + [float(x) for x in r_lo[1:]]

    # 6) å¯é€‰ä¸¢å¼ƒï¼šå¢é‡â‰ˆ0 çš„æœ«è¡Œï¼ˆæ¯”å¦‚ 'const(out)'ï¼‰é¿å…ç”Ÿæˆç©ºå±‚
    names_raw = df[names_col].astype(str).tolist() if (names_col in df.columns and names_col) else [f"L{j}" for j in range(len(dL))]
    if drop_const_out:
        keep = [(i == 0) or (dL[i] > 0.0) for i in range(len(dL))]
        inner_radii = [inner_radii[i] for i, k in enumerate(keep) if k]
        dL_list     = [float(dL[i])     for i, k in enumerate(keep) if k]
        names       = [names_raw[i]     for i, k in enumerate(keep) if k]
    else:
        dL_list = [float(x) for x in dL]
        names   = names_raw

    return inner_radii, dL_list, names


def build_film_from_csv(
    sim,
    csv_path: str = "film/film_layers_final_codesign.csv",
    *,
    nz_per_layer: int = 4,
    dr_fwhm: float = 40e-6,     # è½¯è¾¹ç¼˜FWHMï¼ˆæ¨è 30â€“60 Âµmï¼‰
    aperture: str | float | None = "3w",  # '3w' | 'from_csv' | æ•°å€¼(ç±³) | None
    **csv_kwargs                   # é€ä¼ ç»™ csv_to_layers_listsï¼Œä¾‹å¦‚ col_thickness='t_flat (mm)'
):
    """
    è¯»å– CSV â†’ inner_radii, dL_list â†’ æ„å»º sim.stepped_film
    """
    inner_radii, dL_list, names = csv_to_layers_lists(csv_path, **csv_kwargs)

    # é€‰æ‹©å¤–å…‰é˜‘
    aperture_radius = None
    aperture_factor = None
    if aperture == "3w":
        aperture_radius = None
        aperture_factor = 3.0
    elif aperture == "from_csv":
        # ç”¨CSVæœ€åä¸€è¡Œçš„rä¸Šç•Œï¼›éœ€è¦å†è§£æä¸€æ¬¡
        df = pd.read_csv(csv_path)
        last_hi_m = _parse_r_range_mm_to_m(df[csv_kwargs.get("col_range", "r range (mm)")].iloc[-1])[1]
        aperture_radius = float(last_hi_m)
        aperture_factor = None
    elif isinstance(aperture, (int, float)):
        aperture_radius = float(aperture)
        aperture_factor = None
    else:
        aperture_radius = None
        aperture_factor = None

    film = sim.set_stepped_film_from_annuli(
        inner_radii=inner_radii,
        dL_list=dL_list,
        nz_per_layer=nz_per_layer,
        dr_fwhm=dr_fwhm,
        aperture_radius=aperture_radius,
        aperture_factor=aperture_factor,
        names=names,
    )
    return film, inner_radii, dL_list, names


def set_film_simple(sim,
                    r,              # ä¾‹å¦‚ [1,2,3]  â€”â€” åŠå¾„åˆ†ç•Œï¼ˆé»˜è®¤ mmï¼‰
                    t,              # ä¾‹å¦‚ [0.1,0.2,0.3] â€”â€” å„åŒºâ€œç»å¯¹åšåº¦â€ï¼ˆé»˜è®¤ mmï¼‰
                    *,
                    units="mm",     # "mm" æˆ– "m"
                    nz_per_layer=4, # æ¯å±‚çš„çºµå‘æ­¥æ•°
                    dr_fwhm=40e-6,  # è½¯è¾¹ FWHMï¼ˆç±³ï¼‰ï¼Œ=0 åˆ™ç¡¬è¾¹
                    aperture_from_last=True,   # ç”¨æœ€åä¸€ä¸ªåŠå¾„å½“åšå¤–å…‰é˜‘
                    names=None,     # å¯é€‰ï¼šæ¯å±‚åå­—
                    clip_negative=True,  # ç»å¯¹åšåº¦è‹¥éå•è°ƒï¼ŒæŠŠè´Ÿå¢é‡å¤¹æˆ0
                    interpret="absolute" # "absolute"=tæ˜¯ç»å¯¹åšåº¦ï¼›"delta"=tå°±æ˜¯å¢é‡
                   ):
    """
    0â€“r[0]ã€r[0]â€“r[1]ã€... è¿™äº›åŒºåŸŸçš„åšåº¦ç”± t[0], t[1], ... ç»™å®šã€‚
    å†…éƒ¨ä¼šè‡ªåŠ¨æŠŠâ€œç»å¯¹åšåº¦â€è½¬æ¢æˆå¢é‡ Î”Lï¼Œä»¥é€‚é… set_stepped_film_from_annuliã€‚
    """
    r = [float(x) for x in r]
    t = [float(x) for x in t]
    assert len(r) == len(t), "r å’Œ t çš„é•¿åº¦å¿…é¡»ä¸€è‡´ï¼ˆæ¯ä¸ªåˆ†ç•ŒåŠå¾„å¯¹åº”ä¸€ä¸ªåšåº¦å€¼ï¼‰"
    # å•ä½æ¢ç®—
    if units == "mm":
        r_m = [x * 1e-3 for x in r]
        t_m = [x * 1e-3 for x in t]
    elif units == "m":
        r_m = r
        t_m = t
    else:
        raise ValueError("units å¿…é¡»æ˜¯ 'mm' æˆ– 'm'")

    K = len(t_m)

    # æŠŠç»å¯¹åšåº¦ t â†’ å¢é‡ dLï¼ˆç´¯åŠ å¼å°é˜¶ï¼‰ï¼š
    if interpret == "absolute":
        dL = [t_m[0]] + [t_m[i] - t_m[i-1] for i in range(1, K)]
    elif interpret == "delta":
        dL = list(t_m)
    else:
        raise ValueError("interpret åªèƒ½æ˜¯ 'absolute' æˆ– 'delta'")

    if clip_negative:
        # å’Œä½  csv ç‰ˆæœ¬ä¸€è‡´ï¼šè‹¥åšåº¦éå•è°ƒï¼Œè´Ÿå¢é‡å¤¹åˆ° 0
        dL = [x if x >= 0.0 else 0.0 for x in dL]

    # inner_radii çš„é•¿åº¦å¿…é¡»ç­‰äºå±‚æ•° K
    # ç¬¬ä¸€å±‚ç”¨ None è¡¨ç¤ºâ€œæ•´ç‰‡â€ï¼Œåé¢çš„ä» r[0], r[1], ... ä¾æ¬¡å¼€å§‹ç´¯åŠ 
    inner_radii = [None] + r_m[:-1] if K > 0 else []

    # å¯é€‰ï¼šæŠŠæœ€åä¸€ä¸ªåŠå¾„å½“åšå¤–å…‰é˜‘ï¼ˆé¿å…æœ€å¤–å±‚æ— é™å»¶ä¼¸ï¼‰
    aperture_radius = r_m[-1] if aperture_from_last else None

    # ç¼ºçœåå­—
    if names is None:
        names = [f"step{j}" for j in range(K)]

    # ç›´æ¥ç”¨ä½ ç°æœ‰çš„å…¥å£æ„å»º
    film = sim.set_stepped_film_from_annuli(
        inner_radii=inner_radii,
        dL_list=dL,
        nz_per_layer=nz_per_layer,
        dr_fwhm=dr_fwhm,
        aperture_radius=aperture_radius,
        names=names,
    )
    return film

import pandas as pd, numpy as np, torch
import re

def _parse_r_range_mm_to_m(s: str) -> tuple[float, float]:
    parts = re.split(r"\s*[-â€“â€”]\s*", str(s).strip())
    if len(parts) != 2: raise ValueError(f"Bad r range string: {s!r}")
    lo_mm, hi_mm = float(parts[0]), float(parts[1])
    return lo_mm*1e-3, hi_mm*1e-3

def build_mirror_height_map(sim,
                            csv_path="film/mirror_echelon_final_codesign.csv",
                            *,
                            col_range="r range (mm)",
                            col_height="d_abs_final (Âµm)",
                            dr_fwhm=40e-6,
                            aperture="3w",            # '3w' | 'from_csv' | float(m) | None
                            eps_um=1e-3,              # å°äºè¯¥é˜ˆå€¼(Âµm)çš„å¢é‡è§†ä¸º0ï¼ˆå»æŠ–ï¼‰
                            sort_rows=True            # å…ˆæŒ‰ r_lo å‡åº
                            ):
    """
    ä»é•œé¢å°é˜¶ CSV ç”Ÿæˆ H(x,y)ï¼ˆå•ä½ mï¼‰ã€‚
    ä¿®æ­£ç‚¹ï¼š
      â€¢ aperture='3w' ç°åœ¨ä¼šæŒ‰ 3*w è‡ªåŠ¨è®¡ç®—åŠå¾„ï¼ˆå¹¶å¤¹åˆ°ç½‘æ ¼å†…ï¼‰
      â€¢ å¢é‡ dH å…è®¸æ­£è´Ÿï¼Œåªå»é™¤æå°æŠ–åŠ¨
      â€¢ å¯é€‰ï¼šæŒ‰ r_lo å‡åºæ’åº
    """
    import pandas as pd, numpy as np, torch

    df = pd.read_csv(csv_path)

    # 1) è§£æåŠå¾„åŒºé—´ (mmâ†’m)
    ranges_m = [ _parse_r_range_mm_to_m(s) for s in df[col_range].astype(str).tolist() ]
    r_lo = np.array([lo for lo, _ in ranges_m], dtype=float)
    r_hi = np.array([hi for _, hi in ranges_m], dtype=float)

    # 2) ç»å¯¹é«˜åº¦ (Âµmâ†’m)
    h_abs = df[col_height].astype(float).to_numpy() * 1e-6

    # 3) æŒ‰ r_lo å‡åºæ’åºï¼ˆé¿å…é¡ºåºé”™è¯¯ï¼‰
    if sort_rows:
        order = np.argsort(r_lo)
        r_lo, r_hi, h_abs = r_lo[order], r_hi[order], h_abs[order]

    # 4) ç»å¯¹â†’å¢é‡ï¼ˆå…è®¸æ­£è´Ÿï¼‰ï¼Œåªå»è¶…å°æŠ–åŠ¨
    dH = np.empty_like(h_abs)
    dH[0]  = h_abs[0]
    dH[1:] = h_abs[1:] - h_abs[:-1]
    eps_m = float(eps_um) * 1e-6
    dH[np.abs(dH) < eps_m] = 0.0   # å»æŠ–ï¼›ä¸å‰ªè´Ÿå·ï¼

    # 5) å†…åŠå¾„ï¼šç¬¬ä¸€å±‚ç”¨ None è¡¨ç¤ºæ•´ç‰‡ï¼Œå…¶åç”¨æ¯è¡Œä¸‹ç•Œ
    inner_radii = [None] + [float(x) for x in r_lo[1:]]

    # 6) å¤–å…‰é˜‘åŠå¾„
    if aperture == "3w":
        w = sim._infer_beam_radius() or 0.0
        if w > 0:
            aperture_radius = 3.0 * float(w)
        else:
            # é€€åŒ–ï¼šå– CSV æœ€åä¸€è¡Œä¸Šç•Œ
            aperture_radius = float(r_hi[-1])
    elif aperture == "from_csv":
        aperture_radius = float(r_hi[-1])
    elif isinstance(aperture, (int, float)):
        aperture_radius = float(aperture)
    else:
        aperture_radius = None

    # é™å¹…åˆ°ç½‘æ ¼
    if aperture_radius is not None:
        r_grid = float(min(sim.x.abs().max(), sim.y.abs().max()))
        aperture_radius = min(aperture_radius, 0.98 * r_grid)

    # 7) æ©è†œï¼ˆè½¯è¾¹ï¼‰
    masks = make_annular_masks(sim.X, sim.Y,
                               inner_radii=inner_radii,
                               dr_fwhm=dr_fwhm,
                               aperture_radius=aperture_radius,
                               aperture_fwhm=dr_fwhm)

    # 8) å åŠ ï¼šH(x,y) = Î£ dH_j Â· M_j
    Hxy = torch.zeros_like(sim.X, dtype=torch.float32, device=sim.X.device)
    for d, M in zip(dH, masks):
        Hxy.add_(M, alpha=float(d))
    return Hxy



# -------------------------------
# Mask utilities (soft edges)
# -------------------------------

def _sigma_from_fwhm(dr_fwhm: float) -> float:
    """FWHM (m) â†’ Gaussian sigma (m) for erf-based soft step."""
    # FWHM = 2*sqrt(2*ln2)*sigma
    return dr_fwhm / (2.0 * (2.0 * np.log(2.0))**0.5)

def soft_unit_step(x: torch.Tensor, fwhm: float) -> torch.Tensor:
    """
    Smooth Heaviside H(x) with transition ~ FWHM.
    Returns value in [0,1]. fwhm in meters.
    """
    if fwhm <= 0:
        return (x >= 0).to(torch.float32)
    sigma = _sigma_from_fwhm(fwhm)
    t = x / (np.sqrt(2.0) * sigma)
    return 0.5 * (1.0 + torch.erf(t))



def make_annular_masks(
    X: torch.Tensor, Y: torch.Tensor,
    inner_radii: Sequence[float],         # r_edge for each layer; j=0 å¯ç”¨ None/è´Ÿæ•° è¡¨ç¤ºâ€œæ•´ç‰‡ä¸º1â€
    dr_fwhm: float,                        # edge smoothing FWHM [m], e.g. 30e-6
    aperture_radius: float | None = None,  # optional hard/soft outer stop
    aperture_fwhm: float | None = None,    # soft edge for aperture if given
) -> List[torch.Tensor]:
    """
    Generate M_j(x,y) for K layers where layer j occupies r >= inner_radii[j].
    inner_radii[j] can be None (or <0) to indicate a full disk (M=1 everywhere).
    Returns masks as float32 tensors on the same device as X/Y.
    """
    r = torch.sqrt(X**2 + Y**2)
    masks: List[torch.Tensor] = []
    for j, r_edge in enumerate(inner_radii):
        if (r_edge is None) or (r_edge < 0):
            Mj = torch.ones_like(r, dtype=torch.float32)
        else:
            Mj = soft_unit_step(r - float(r_edge), dr_fwhm).to(torch.float32)

        # optional outer aperture
        if aperture_radius is not None:
            if aperture_fwhm is None or aperture_fwhm <= 0:
                M_ap = (r <= aperture_radius).to(torch.float32)
            else:
                M_ap = soft_unit_step(aperture_radius - r, aperture_fwhm).to(torch.float32)
            Mj = Mj * M_ap

        masks.append(Mj)
    return masks



from typing import Literal, Optional
import time

def human(mem_bytes):
    for unit in ['B','KB','MB','GB']:
        if mem_bytes < 1024.0:
            return f"{mem_bytes:.1f}{unit}"
        mem_bytes /= 1024.0
    return f"{mem_bytes:.1f}TB"

# -----------------------------------------------------------------------------
# Helper functions â€”â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“
# -----------------------------------------------------------------------------

def _auto_limits(x: np.ndarray, y: np.ndarray, zoom: Optional[float] = 6.0):
    """Return (xmin, xmax) so that the shown span â‰ˆ *zoom* Ã— FWHM of *y*.
    If *zoom* is *None* the full axis is returned.
    """
    if zoom is None:
        return x.min(), x.max()

    y = np.abs(y)
    if y.max() == 0:
        return x.min(), x.max()

    half = 0.5 * y.max()
    idx = np.where(y >= half)[0]
    if len(idx) < 2:
        return x.min(), x.max()

    x0, x1 = x[idx[0]], x[idx[-1]]
    cx = 0.5 * (x0 + x1)
    w = (x1 - x0) * 0.5 * zoom
    return cx - w, cx + w

# -----------------------------------------------------------------------------
# Main visualisation routine â€”â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“
# -----------------------------------------------------------------------------
@torch.inference_mode()
def visualize_simulator(
    sim,  # PulseGNLSESimulator instance
    *,
    field: Literal["input", "current", "output"] = "current",
    zoom_time: Optional[float] = 3.0,
    zoom_space: Optional[float] = 12.0,
    log_scale_2d: bool = False,
):
    """Create a 3Ã—2 Matplotlib figure visualising the given *sim* state.

    Parameters
    ----------
    sim : PulseGNLSESimulator
        The simulator object.
    field : {'input','current','output'}
        Which electricâ€‘field snapshot to visualise.  *current* uses ``sim.A``.
    zoom_time, zoom_space : float | None
        Multiple of the FWHM/RMS width kept visible along the time and spatial axes.
        Set to *None* to show the full window.
    log_scale_2d : bool, default False
        If *True*, the 2â€‘D heat maps are coloured on a log10 scale.

    Returns
    -------
    matplotlib.figure.Figure
        The created figure object (already populated).
    """
    # -------- choose which field ------------------------------------

    # -------- 2) ç»Ÿâ€‘ä¸€â€‘æ¬â€‘åˆ° CPU (å…³é”®ä¸€æ­¥) --------------------------
    T   = sim.T.detach().cpu().numpy()
    x   = sim.x.detach().cpu().numpy()
    y   = sim.y.detach().cpu().numpy()
    omega = sim.omega.detach().cpu().numpy()
    Kx  = sim.Kx.detach().cpu().numpy()
    Ky  = sim.Ky.detach().cpu().numpy()
    dx, dy = float(sim.dx), float(sim.dy)    # pythonâ€‘float
    
    if field == "input":
    
        torch.cuda.empty_cache()
        
        I_kxky = np.trapezoid(np.abs(sim.fft_xy((sim.A_in).to(sim.T.device))[2].detach().cpu().numpy())**2, T, axis=2)
        torch.cuda.empty_cache()
    
        S_w  = np.sum(np.abs(sim.fft_t((sim.A_in).to(sim.T.device))[1].detach().cpu().numpy())**2, axis=(0,1)) * dx * dy  # |A|^2 vs Ï‰
        torch.cuda.empty_cache()
    
        I =np.abs( (sim.A_in).to(sim.T.device).detach().cpu().numpy() )**2       # (Nx,Ny,Nt), complex
   
    elif field == "output":
        if sim.A_out is None:
            raise ValueError("sim.A_out is None â€” run sim.propagate() first or choose another field")

        torch.cuda.empty_cache()
        
        I_kxky = np.trapezoid(np.abs(sim.fft_xy((sim.A_out).to(sim.T.device))[2].detach().cpu().numpy())**2, T, axis=2)
        torch.cuda.empty_cache()
    
        S_w  = np.sum(np.abs(sim.fft_t((sim.A_out).to(sim.T.device))[1].detach().cpu().numpy())**2, axis=(0,1)) * dx * dy  # |A|^2 vs Ï‰
        
        torch.cuda.empty_cache()
    
        I =np.abs( (sim.A_out).to(sim.T.device).detach().cpu().numpy() )**2        # (Nx,Ny,Nt), complex
    else:
    
        torch.cuda.empty_cache()
        
        I_kxky = np.trapezoid(np.abs(sim.fft_xy(sim.A)[2].detach().cpu().numpy())**2, T, axis=2)
        torch.cuda.empty_cache()
    
        S_w  = np.sum(np.abs(sim.fft_t(sim.A)[1].detach().cpu().numpy())**2, axis=(0,1)) * dx * dy  # |A|^2 vs Ï‰

        torch.cuda.empty_cache()
    
        I =np.abs( sim.A.detach().cpu().numpy() )**2
 
    
    # ---------- é¢‘åŸŸä¸­å¿ƒåŒ– ----------------------------------------
    omega = np.fft.fftshift(omega)          # (Nt,)
    S_w   = np.fft.fftshift(S_w)            # (Nt,)
    # --------------------------------------------------------------
    
    # ---------- kâ€‘space ä¸­å¿ƒåŒ– -------------------------------------
    I_kxky = np.fft.fftshift(I_kxky, axes=(0, 1))
    Kx     = np.fft.fftshift(Kx,   axes=0)
    Ky     = np.fft.fftshift(Ky,   axes=1)
    torch.cuda.empty_cache()
    

    # ===== 1D temporal power ========================================
    P_t = np.sum(I, axis=(0,1)) * dx * dy


    
    # ===== 1D spectral power (use direct FFT of full field) =========
    '''
    _,  A_w  = sim.fft_t(A_gpu)           # â† è¿™è¡Œä»åœ¨ GPU åš FFT
    A_w  = sim.fft_t(A_gpu)[1].detach().cpu().numpy()
    '''
    #S_w  = np.sum(np.abs( A_w)**2, axis=(0,1)) * dx * dy  # |A|^2 vs Ï‰
    
    
    # ===== 2D transverse intensity ==================================
    I_xy = np.trapezoid(I,T, axis=2)  # integrate over t

    torch.cuda.empty_cache()

    # ===== 2D kâ€‘space intensity =====================================
    '''
    _, _, A_k = sim.fft_xy(A_gpu)
    A_k  = A_k.detach().cpu().numpy()
    I_kxky = np.trapezoid(np.abs(A_k)**2, T, axis=2)
    '''
    

    # ===== 1D transverse profile (x) ================================
    idx_y0=np.argmin(np.abs(y))
    I_x = I_xy[:,idx_y0]  # integrate over y

    # ===== 1D kâ€‘space profile (kx) ==================================
    idx_ky0=np.argmin(np.abs(Ky[0, :]))
    I_kx = I_kxky[:,idx_ky0]

    # ----------------------------------------------------------------
    # Figure layout  â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“â€“
    # ----------------------------------------------------------------
    fig, axes = plt.subplots(3, 2, figsize=(12, 12))
    (ax_t, ax_w), (ax_xy, ax_k), (ax_x, ax_kx) = axes

    # ---- Time domain ----------------------------------------------
    ax_t.plot(T, P_t)
    ax_t.set_xlabel("t  [s]")
    ax_t.set_ylabel("âˆ«|A|Â² dx dy  [J/s]")
    ax_t.set_title("Temporal power (integrated over x,y)")
    ax_t.set_xlim(_auto_limits(T, P_t, zoom_time))


    # ---- Frequency domain -----------------------------------------
    ax_w.plot(omega, S_w)
    ax_w.set_xlabel("Ï‰  [rad/s]")
    ax_w.set_ylabel("âˆ«|Ã‚|Â² dx dy  [arb]")
    ax_w.set_title("Spectral power (integrated over x,y)")
    ax_w.set_xlim(_auto_limits(omega, S_w, zoom_time))

    # ---- 2â€‘D spatial map ------------------------------------------
    extent_xy = [x.min(), x.max(), y.min(), y.max()]
    if log_scale_2d:
        im_xy = ax_xy.imshow(
            np.log10(np.maximum(I_xy, I_xy.max() * 1e-12)),  # avoid log(0)
            origin="lower",
            extent=extent_xy,
            aspect="equal",
        )
        cbar_label = "logâ‚â‚€ I(x,y)"
    else:
        im_xy = ax_xy.imshow(
            I_xy,
            origin="lower",
            extent=extent_xy,
            aspect="equal",
        )
        cbar_label = "I(x,y)  [arb]"
    fig.colorbar(im_xy, ax=ax_xy, label=cbar_label)
    ax_xy.set_xlabel("x  [m]")
    ax_xy.set_ylabel("y  [m]")
    ax_xy.set_title("Spatial intensity (tâ€‘integrated)")
    # zoom spatial axes
    x_lim = _auto_limits(x, I_x, zoom_space)
    y_proj = np.trapezoid(I_xy, x, axis=0)
    y_lim = _auto_limits(y, y_proj, zoom_space)
    ax_xy.set_xlim(x_lim)
    ax_xy.set_ylim(y_lim)

    # ---- 2â€‘D kâ€‘space map -----------------------------------------
    extent_k = [Kx.min(), Kx.max(), Ky.min(), Ky.max()]
    if log_scale_2d:
        im_k = ax_k.imshow(
            np.log10(np.maximum(I_kxky, I_kxky.max() * 1e-12)),
            origin="lower",
            extent=extent_k,
            aspect="equal",
        )
        cbar_label_k = "logâ‚â‚€ I(kx,ky)"
    else:
        im_k = ax_k.imshow(
            I_kxky,
            origin="lower",
            extent=extent_k,
            aspect="equal",
        )
        cbar_label_k = "I(kx,ky)  [arb]"
    fig.colorbar(im_k, ax=ax_k, label=cbar_label_k)
    ax_k.set_xlabel("kâ‚“  [mâ»Â¹]")
    ax_k.set_ylabel("k_y  [mâ»Â¹]")
    ax_k.set_title("Spatial spectrum (tâ€‘integrated)")
    Kx_lim = _auto_limits(Kx[:, 0], I_kx, zoom_space)
    ky_proj = np.trapezoid(I_kxky, Kx[:, 0], axis=0)
    Ky_lim = _auto_limits(Ky[0, :], ky_proj, zoom_space)
    ax_k.set_xlim(Kx_lim)
    ax_k.set_ylim(Ky_lim)


    # ---- 1â€‘D spatial profile (x) ----------------------------------
    ax_x.plot(x, I_x)
    ax_x.set_xlabel("x  [m]")
    ax_x.set_ylabel("|A|Â² at yâ‰ˆ0  [arb]")
    ax_x.set_title("Line profile at yâ‰ˆ0")
    ax_x.set_xlim(x_lim)

    # ---- 1â€‘D kâ€‘space profile (kx) ---------------------------------
    ax_kx.plot(Kx[:, 0], I_kx)
    ax_kx.set_xlabel("kâ‚“  [mâ»Â¹]")
    ax_kx.set_ylabel("âˆ«|Ã‚|Â² dky dÏ‰  [arb]")
    ax_kx.set_title("kâ€‘space line (integrated over ky,Ï‰)")
    ax_kx.set_xlim(Kx_lim)

    fig.tight_layout()
    plt.show()
    return fig

@torch.inference_mode()
def film_thickness_map(sim) -> torch.Tensor:
    """
    Return L(x,y) = sum_j dL_j * M_j(x,y)
    dtype=float32, device=sim.X.device, shape=(Nx,Ny)
    """
    assert sim.stepped_film is not None, "sim.stepped_film is None"
    dev = sim.X.device
    Lxy = torch.zeros((sim.Nx, sim.Ny), dtype=torch.float32, device=dev)
    for layer in sim.stepped_film.layers:
        # æ˜¾å¼æŒ‰å±‚ç´¯åŠ ï¼šL += Î”L_j * M_j
        Lxy.add_(layer.mask, alpha=float(layer.dL))
    return Lxy


@torch.inference_mode()
def visualize_film(sim, *, crop_factor: float = 2.0, line_axis: str = 'x'):
    """
    å·¦ï¼šåšåº¦å›¾ L(x,y)
    ä¸­ï¼šä¸­å¿ƒæˆªçº¿åšåº¦ï¼ˆé»˜è®¤ y=0ï¼›line_axis='y' åˆ™ç”» x=0 çš„å‚ç›´æˆªçº¿ï¼‰
    å³ï¼šä¸­å¿ƒæˆªçº¿â€œç›¸å¯¹å…¨ç©ºæ°”â€çš„ç†è®ºç¾¤æ—¶å»¶ Î”t(x) = [Î²1_film - Î²1_ref] * L(x)ï¼Œå•ä½ fs
    """
    assert sim.stepped_film is not None, "sim.stepped_film is None"

    # 1) åšåº¦å›¾ = âˆ‘ dL_j * M_j
    Lxy = film_thickness_map(sim)                 # (Nx,Ny) [m]

    # 2) ç½‘æ ¼ä¸æˆªçº¿ç´¢å¼•
    x = sim.x.detach().cpu().numpy()
    y = sim.y.detach().cpu().numpy()
    L = Lxy.detach().cpu().numpy()

    if line_axis.lower() == 'x':
        # å– yâ‰ˆ0 çš„é‚£ä¸€è¡Œ
        iy0 = int(torch.argmin(torch.abs(sim.y)).item())
        x_line = x
        L_line = L[:, iy0]
        xlabel = "x [m]"
    else:
        # å– xâ‰ˆ0 çš„é‚£ä¸€åˆ—
        ix0 = int(torch.argmin(torch.abs(sim.x)).item())
        x_line = y
        L_line = L[ix0, :]
        xlabel = "y [m]"

    # 3) è§†é‡èŒƒå›´ï¼šÂ±(crop_factor * w)ï¼›è‹¥å–ä¸åˆ° wï¼Œåˆ™ç”¨ 0.8Ã—ç½‘æ ¼åŠå®½
    try:
        w = sim._infer_beam_radius()
    except Exception:
        w = None
    r_grid = float(min(np.max(np.abs(x)), np.max(np.abs(y))))
    lim = float(min(crop_factor * w, 0.98 * r_grid)) if (w is not None and w > 0) else 0.8 * r_grid

    # === æ–°å¢ï¼šç†è®ºç¾¤æ—¶å»¶ï¼ˆç›¸å¯¹â€œå…¨ç©ºæ°”â€ï¼‰===========================
    # Î²1 = dÎ²/dÏ‰ï¼Œå•ä½ s/mï¼›å·²åœ¨ sim.materials ä¸­ç»™å‡ºï¼š
    #   beta_film[1] æ¥è‡ªä½ çš„ dispersion å­—å…¸ï¼›beta_ref[1]â‰ˆ1/c
    beta1_film = float(sim.materials["beta_film"][1])   # s/m
    beta1_ref  = float(sim.materials["beta_ref" ][1])   # s/m
    delta_beta1 = beta1_film - beta1_ref                # s/m
    tau_line_fs = (delta_beta1 * L_line) * 1e15         # fs

    # 4) ç”»å›¾ï¼šä¸‰è”å›¾
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))

    # å·¦ï¼šL(x,y)
    im = ax1.imshow(
        L, origin="lower",
        extent=[x.min(), x.max(), y.min(), y.max()],
        aspect="equal"
    )
    ax1.set_title("Film thickness $L(x,y)$ [m]")
    fig.colorbar(im, ax=ax1)
    ax1.set_xlabel("x [m]"); ax1.set_ylabel("y [m]")
    ax1.set_xlim(-lim, lim); ax1.set_ylim(-lim, lim)

    # ä¸­ï¼šä¸­å¿ƒæˆªçº¿åšåº¦ï¼ˆmmï¼‰
    ax2.plot(x_line, L_line * 1e3)
    ax2.set_xlabel(xlabel); ax2.set_ylabel("Thickness [mm]")
    ax2.set_title("Center-line thickness")
    ax2.set_xlim(-lim, lim)
    ax2.grid(True)

    # å³ï¼šä¸­å¿ƒæˆªçº¿â€œç›¸å¯¹ç©ºæ°”â€çš„ç†è®ºç¾¤æ—¶å»¶ï¼ˆfsï¼‰
    ax3.plot(x_line, tau_line_fs)
    ax3.set_xlabel(xlabel); ax3.set_ylabel("Î”t [fs]  (vs air)")
    ax3.set_title("Center-line group delay (theory)")
    ax3.set_xlim(-lim, lim)
    ax3.grid(True)

    fig.tight_layout(); plt.show()


import torch
import numpy as np
import matplotlib.pyplot as plt

@torch.inference_mode()
def visualize_time_and_freq_by_regions(
    sim,
    *,
    field: str = "output",   # 'output' | 'current' | 'input'
    t_unit: str = "fs",      # 'fs' | 'ps' | 's'
    f_unit: str = "THz",     # 'THz' | 'rad/s'
    normalize: bool = False, # æ—¶é—´/é¢‘åŸŸåˆ†åˆ«å„è‡ªå½’ä¸€åŒ–åˆ°æœ€å¤§å€¼=1
    fftshift: bool = True,   # é¢‘è°±å±…ä¸­æ˜¾ç¤º
    show_energy_fraction: bool = True,  # å›¾ä¾‹æ˜¾ç¤ºå„åˆ†åŒºèƒ½é‡å æ¯”
    title: str = "Temporal & Spectral power by spatial regions (GPU, 2-field peak)",
):
    dev = sim.T.device
    if dev.type != "cuda":
        print("[Note] å½“å‰è®¾å¤‡é CUDAï¼Œå°†åœ¨ CPU ä¸Šè¿è¡Œï¼ˆåŠŸèƒ½æ­£ç¡®ä½†é€Ÿåº¦è¾ƒæ…¢ï¼‰ã€‚")
    torch.cuda.empty_cache()
    check_mem()
    # -------- å–åœºå¹¶æ¬åˆ° GPU ----------
    if field == "output":
        if sim.A_out is None:
            raise ValueError("sim.A_out is None â€” è¯·å…ˆå®Œæˆä¼ æ’­ï¼ˆpropagate_with_film æˆ– propagateï¼‰")
        A = (sim.A_out if torch.is_tensor(sim.A_out) else torch.as_tensor(sim.A_out)).to(
            dev, dtype=torch.complex64, non_blocking=True
        )
    elif field == "input":
        A = (sim.A_in if torch.is_tensor(sim.A_in) else torch.as_tensor(sim.A_in)).to(
            dev, dtype=torch.complex64, non_blocking=True
        )
    else:  # 'current'
        if sim.A is None:
            raise ValueError("sim.A is None â€” åªæœ‰ä¼ æ’­è¿‡ç¨‹ä¸­æ‰æœ‰ current åœºï¼›å¦åˆ™è¯·ç”¨ 'output' æˆ– 'input'")
        A = (sim.A if torch.is_tensor(sim.A) else torch.as_tensor(sim.A)).to(
            dev, dtype=torch.complex64, non_blocking=True
        )

    Nx, Ny, Nt = A.shape
    dx, dy = float(sim.dx), float(sim.dy)
    area_scale = dx * dy

    # -------- æ„é€ äº’æ–¥åˆ†åŒºæ©æ¨¡ï¼ˆGPUï¼Œfloat32ï¼‰ ----------
    assert sim.stepped_film is not None and len(sim.stepped_film.layers) > 0, "stepped_film æœªè®¾ç½®æˆ–ä¸ºç©º"
    cum = [L.mask.to(device=dev, dtype=torch.float32, non_blocking=True) for L in sim.stepped_film.layers]
    if len(cum) == 1:
        excl = [cum[0]]
    else:
        excl = [torch.clamp(cum[j] - cum[j + 1], 0.0, 1.0) for j in range(len(cum) - 1)]
        excl.append(torch.clamp(cum[-1], 0.0, 1.0))
    M0 = cum[0]
    torch.cuda.empty_cache()

    # ==========================
    # 1) æ—¶é—´åŸŸ â€” å³°å€¼å†…å­˜ï¼šA + I
    # ==========================
    P_total = torch.zeros(Nt, device=dev, dtype=torch.float32)
    P_regions = [torch.zeros(Nt, device=dev, dtype=torch.float32) for _ in excl]
    
    tile_x = 32  # æˆ– 64/128ï¼Œè§†æ˜¾å­˜è°ƒ
    for xs in range(0, Nx, tile_x):
        xe = min(xs + tile_x, Nx)
        Ab = A[xs:xe, :, :]  # (tx, Ny, Nt), complex64
        Ib = (Ab.real * Ab.real + Ab.imag * Ab.imag).to(torch.float32)  # åªå»ºå°å—
        P_total += torch.einsum('xyt,xy->t', Ib, M0[xs:xe, :]) * area_scale
        for j, Mj in enumerate(excl):
            P_regions[j] += torch.einsum('xyt,xy->t', Ib, Mj[xs:xe, :]) * area_scale
        del Ib
    # è¿™é‡Œ Ib å·²å…¨é‡Šæ”¾ï¼Œä¸ä¼šè§¦å‘ 4GB æ®µ
    torch.cuda.synchronize()  # å¯é€‰ï¼šè®©ç»Ÿè®¡æ›´ç¨³å®š
    torch.cuda.empty_cache()

    # â€”â€” ç«‹åˆ»æ¬åˆ° CPUï¼Œé¿å…åç»­é˜¶æ®µè¿™äº›å°å¼ é‡é’‰ä½å¤§æ®µ â€”â€” #
    T = sim.T.detach().cpu().numpy()
    omega = sim.omega.detach().cpu().numpy()
    P_total_np = P_total.detach().cpu().numpy()
    P_regions_np = [p.detach().cpu().numpy() for p in P_regions]

    # å¦‚éœ€æœ€å¤§é™åº¦æ¸…ç†ï¼Œä¹Ÿå¯æŠŠæ©è†œå…ˆåˆ ï¼Œåé¢é¢‘åŸŸå†é‡å»º
    del P_total, P_regions, M0, cum, excl
    torch.cuda.empty_cache()
    # ==========================
    # 2) é¢‘åŸŸï¼ˆåˆ†å—æµå¼ï¼›ä¸å»º 4GB ç¼“å†²ï¼‰
    # ==========================

    scale_fft_t = float(sim._scale_fft_t.detach().cpu().item())
    A_w = torch.fft.fft(A, dim=-1)  # complex64
    del A
    torch.cuda.empty_cache()
    
    A_w.mul_(scale_fft_t)

    # é‡æ–°æ‹¿æ©è†œï¼ˆGPU float32ï¼‰
    cum = [L.mask.to(device=dev, dtype=torch.float32, non_blocking=True) for L in sim.stepped_film.layers]
    if len(cum) == 1:
        excl = [cum[0]]
    else:
        excl = [torch.clamp(cum[j] - cum[j + 1], 0.0, 1.0) for j in range(len(cum) - 1)]
        excl.append(torch.clamp(cum[-1], 0.0, 1.0))
    M0 = cum[0]

    S_total = torch.zeros(Nt, device=dev, dtype=torch.float32)
    S_regions = [torch.zeros(Nt, device=dev, dtype=torch.float32) for _ in excl]

    tile_x = 32  # å¯è°ƒï¼š32/64/128
    for xs in range(0, Nx, tile_x):
        xe = min(xs + tile_x, Nx)
        Aw_blk = A_w[xs:xe, :, :]  # (tx, Ny, Nt)

        pow_blk = (Aw_blk.real * Aw_blk.real + Aw_blk.imag * Aw_blk.imag).to(torch.float32)

        S_total += torch.sum(pow_blk, dim=(0, 1)) * area_scale
        for j, Mj in enumerate(excl):
            S_regions[j] += torch.einsum('xyt,xy->t', pow_blk, Mj[xs:xe, :]) * area_scale

        del pow_blk

    del A_w
    torch.cuda.empty_cache()
    
    # â€”â€” æ¬åˆ° CPU â€”â€” 
    S_total_np   = S_total.detach().cpu().numpy()
    S_regions_np = [s.detach().cpu().numpy() for s in S_regions]
    del S_total, S_regions, M0, cum, excl
    torch.cuda.empty_cache()

    # ===== å½’ä¸€åŒ–ï¼ˆæ—¶é—´/é¢‘åŸŸå„è‡ªï¼‰ =====
    if normalize:
        denom_t = P_total_np.max() if P_total_np.max() > 0 else 1.0
        denom_w = S_total_np.max() if S_total_np.max() > 0 else 1.0
        P_total_np   = P_total_np / denom_t
        P_regions_np = [p / denom_t for p in P_regions_np]
        S_total_np   = S_total_np / denom_w
        S_regions_np = [s / denom_w for s in S_regions_np]
        ylab_t = "Normalized  âˆ«|A|Â² dx dy"
        ylab_w = "Normalized  âˆ«|Ã‚|Â² dx dy"
    else:
        ylab_t = "âˆ«|A|Â² dx dy  [J/s]"
        ylab_w = "âˆ«|Ã‚|Â² dx dy  [arb]"

    # ===== èƒ½é‡å æ¯” =====
    import numpy as _np
    E_total_t = _np.trapz(P_total_np, T)
    frac_t = [(_np.trapz(p, T) / E_total_t * 100.0) if E_total_t > 0 else 0.0 for p in P_regions_np]

    if f_unit.lower() == "thz":
        f = omega / (2 * np.pi * 1e12)
        xlab_w = "Frequency offset [THz]"
    else:
        f = omega
        xlab_w = "Ï‰  [rad/s]"

    if fftshift:
        f_plot = np.fft.fftshift(f)
        S_total_plot   = np.fft.fftshift(S_total_np)
        S_regions_plot = [np.fft.fftshift(s) for s in S_regions_np]
    else:
        f_plot = f
        S_total_plot   = S_total_np
        S_regions_plot = S_regions_np

    E_total_w = np.trapz(S_total_plot, f_plot)
    frac_w = [(np.trapz(s, f_plot) / E_total_w * 100.0) if E_total_w > 0 else 0.0 for s in S_regions_plot]

    # ===== æ¨ªè½´å•ä½ï¼ˆæ—¶é—´ï¼‰ =====
    if t_unit == "fs":
        t = T * 1e15; xlab_t = "Time [fs]"
    elif t_unit == "ps":
        t = T * 1e12; xlab_t = "Time [ps]"
    else:
        t = T; xlab_t = "Time [s]"

    # ===== åŒºåŸŸæ ‡ç­¾ï¼ˆä¸å†ä¾èµ–å·²åˆ é™¤çš„ exclï¼‰ =====
    K_excl = len(S_regions_np)
    if K_excl == 1:
        labels_t = [f"Region 0 â€” {frac_t[0]:.1f}% E"] if show_energy_fraction else ["Region 0"]
        labels_w = [f"Region 0 â€” {frac_w[0]:.1f}% E"] if show_energy_fraction else ["Region 0"]
    else:
        labels_t = [f"Region 0 (center) â€” {frac_t[0]:.1f}% E" if show_energy_fraction else "Region 0 (center)"]
        labels_w = [f"Region 0 (center) â€” {frac_w[0]:.1f}% E" if show_energy_fraction else "Region 0 (center)"]
        for j in range(1, K_excl - 1):
            labels_t.append(f"Region {j} (ring) â€” {frac_t[j]:.1f}% E" if show_energy_fraction else f"Region {j} (ring)")
            labels_w.append(f"Region {j} (ring) â€” {frac_w[j]:.1f}% E" if show_energy_fraction else f"Region {j} (ring)")
        labels_t.append(f"Region {K_excl-1} (outermost) â€” {frac_t[-1]:.1f}% E" if show_energy_fraction else f"Region {K_excl-1} (outermost)")
        labels_w.append(f"Region {K_excl-1} (outermost) â€” {frac_w[-1]:.1f}% E" if show_energy_fraction else f"Region {K_excl-1} (outermost)")

    # ---- ç»˜å›¾ ----
    fig, (ax_t, ax_w) = plt.subplots(1, 2, figsize=(12, 5))
    fig.suptitle(title)

    ax_t.plot(t, P_total_np, label="Total (within aperture)")
    for Pj, lab in zip(P_regions_np, labels_t):
        ax_t.plot(t, Pj, linestyle="--", label=lab)
    ax_t.set_xlabel(xlab_t); ax_t.set_ylabel(ylab_t)
    ax_t.set_title("Temporal power P(t)")
    ax_t.grid(True); ax_t.legend()

    ax_w.plot(f_plot, S_total_plot, label="Total (within aperture)")
    for Sj, lab in zip(S_regions_plot, labels_w):
        ax_w.plot(f_plot, Sj, linestyle="--", label=lab)
    ax_w.set_xlabel(xlab_w); ax_w.set_ylabel(ylab_w)
    ax_w.set_title("Spectral power S(Ï‰)")
    ax_w.grid(True); ax_w.legend()

    fig.tight_layout()
    plt.show()
    return 0


import math
import torch
import numpy as np
import matplotlib.pyplot as plt

@torch.inference_mode()
def visualize_mirror(
    sim,
    Hxy: torch.Tensor | None = None,
    *,
    # è‹¥æœªä¼  Hxyï¼Œå¯ç›´æ¥ä» CSV æ„å»ºï¼š
    csv_path: str | None = None,
    col_range: str = "r range (mm)",
    col_height: str = "d_abs_final (Âµm)",
    dr_fwhm: float = 40e-6,
    aperture: str | float | None = "3w",
    # å…‰å­¦å‚æ•°
    theta_deg: float = 0.0,   # å…¥å°„è§’ï¼ˆåº¦ï¼‰
    n0: float = 1.0,          # ä¼ æ’­ä»‹è´¨æŠ˜å°„ç‡ï¼ˆç©ºæ°”â‰ˆ1ï¼‰
    # ç”»å›¾é€‰é¡¹
    crop_factor: float = 2.0, # è§†é‡ ~ Â±(crop_factor Ã— w)
    line_axis: str = "x"      # 'x' è¡¨ç¤ºå– yâ‰ˆ0 çš„ä¸­å¿ƒçº¿ï¼›'y' å– xâ‰ˆ0
):
    """
    å¯è§†åŒ–åå°„é•œå°é˜¶é«˜åº¦ H(x,y) åŠç”±å‡ ä½•å…‰ç¨‹å¯¼è‡´çš„ç¾¤æ—¶å»¶ Î”t(x,y)ã€‚

    å…¬å¼ï¼š
        L(x,y) = 2 cosÎ¸ Â· H(x,y)
        Ï†0(x,y) = (n0Â·Ï‰0/c) Â· L(x,y)
        Î”t(x,y) = (n0/c) Â· L(x,y) = (2 n0 cosÎ¸ / c) Â· H(x,y)

    å‚æ•°ï¼š
      â€¢ è‹¥å·²åœ¨å¤–éƒ¨æ„å»ºäº† Hxyï¼ˆå•ä½ mï¼‰ï¼Œå¯ç›´æ¥ä¼ å…¥ï¼›
        å¦åˆ™æä¾› csv_pathï¼Œæœ¬å‡½æ•°ä¼šè°ƒç”¨ build_mirror_height_map() è¯»å–å¹¶ç”Ÿæˆã€‚
    """
    # 1) å‡†å¤‡ H(x,y)
    if Hxy is None:
        assert csv_path is not None, "æœªä¼  Hxy æ—¶å¿…é¡»æä¾› csv_path æ‰èƒ½è¯»å–é•œé¢å°é˜¶ CSV"
        Hxy = build_mirror_height_map(sim,
                                      csv_path=csv_path,
                                      col_range=col_range,
                                      col_height=col_height,
                                      dr_fwhm=dr_fwhm,
                                      aperture=aperture)
    Hxy = Hxy.to(sim.X.device, dtype=torch.float32)
    H   = Hxy.detach().cpu().numpy()  # [m]

    # 2) ç½‘æ ¼åæ ‡
    x = sim.x.detach().cpu().numpy()
    y = sim.y.detach().cpu().numpy()

    # 3) é€‰ä¸­å¿ƒçº¿
    if str(line_axis).lower() == "x":
        iy0    = int(torch.argmin(torch.abs(sim.y)).item())
        coord  = x
        H_line = H[:, iy0]
        xlabel = "x [m]"
    else:
        ix0    = int(torch.argmin(torch.abs(sim.x)).item())
        coord  = y
        H_line = H[ix0, :]
        xlabel = "y [m]"

    # 4) è®¡ç®—å‡ ä½•å…‰ç¨‹ä¸ç¾¤æ—¶å»¶ï¼ˆä¸­å¿ƒçº¿ï¼‰
    cos_theta   = math.cos(math.radians(theta_deg))
    L_line      = 2.0 * cos_theta * H_line                # [m]
    tau_line_s  = (n0 / sim.c) * L_line                   # [s]
    tau_line_fs = tau_line_s * 1e15                        # [fs]

    # 5) è§†é‡èŒƒå›´ï¼ˆå°½é‡ä¸ visualize_film ä¸€è‡´ï¼‰
    try:
        w = sim._infer_beam_radius()
    except Exception:
        w = None
    r_grid = float(min(np.max(np.abs(x)), np.max(np.abs(y))))
    lim = float(min(crop_factor * w, 0.98 * r_grid)) if (w is not None and w > 0) else 0.8 * r_grid

    # 6) ä¸‰è”å›¾ï¼šH(x,y)ã€ä¸­å¿ƒçº¿ Hã€ä¸­å¿ƒçº¿ Î”t
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))

    # å·¦ï¼šH(x,y)ï¼ˆé¢œè‰²æ¡ç”¨å¾®ç±³å•ä½ç›´è§‚ä¸€äº›ï¼‰
    im = ax1.imshow(
        H * 1e6, origin="lower",
        extent=[x.min(), x.max(), y.min(), y.max()],
        aspect="equal"
    )
    ax1.set_title("Mirror height $H(x,y)$ [Âµm]")
    fig.colorbar(im, ax=ax1, label="H [Âµm]")
    ax1.set_xlabel("x [m]"); ax1.set_ylabel("y [m]")
    ax1.set_xlim(-lim, lim); ax1.set_ylim(-lim, lim)

    # ä¸­ï¼šä¸­å¿ƒçº¿é«˜åº¦ï¼ˆÂµmï¼‰
    ax2.plot(coord, H_line * 1e6)
    ax2.set_xlabel(xlabel); ax2.set_ylabel("Height [Âµm]")
    ax2.set_title("Center-line height")
    ax2.set_xlim(-lim, lim); ax2.grid(True)

    # å³ï¼šä¸­å¿ƒçº¿ç¾¤æ—¶å»¶ï¼ˆfsï¼‰
    ax3.plot(coord, tau_line_fs)
    ax3.set_xlabel(xlabel); ax3.set_ylabel("Î”t [fs]  (vs flat mirror)")
    ax3.set_title(f"Center-line group delay (n0={n0:g}, Î¸={theta_deg:.1f}Â°)")
    ax3.set_xlim(-lim, lim); ax3.grid(True)

    fig.tight_layout(); plt.show()

    # å¯é€‰è¿”å›ï¼Œä¾¿äºåç»­æ•°å€¼å¤„ç†
    return {
        "Hxy": Hxy.detach().cpu(),
        "coord": coord,
        "H_line_um": H_line * 1e6,
        "tau_line_fs": tau_line_fs,
        "theta_deg": theta_deg,
        "n0": n0
    }

    
import psutil

def check_mem(gpu_id: int = 0):
    """Print GPU æ˜¾å­˜ + CPU å†…å­˜ä½¿ç”¨æƒ…å†µ."""
    # ---------------- GPU ----------------
    props      = torch.cuda.get_device_properties(gpu_id)
    total_mem  = props.total_memory
    reserved   = torch.cuda.memory_reserved(gpu_id)
    allocated  = torch.cuda.memory_allocated(gpu_id)

    free_in_pool = reserved - allocated
    gpu_avail    = total_mem - reserved

    print("======= GPU memory (device", gpu_id, ") =======")
    print(f"Total       : {total_mem/1024**2:8.1f}â€¯MB")
    print(f"Allocated   : {allocated/1024**2:8.1f}â€¯MB")
    print(f"Reserved    : {reserved/1024**2:8.1f}â€¯MB "
          f"(free in pool: {free_in_pool/1024**2:8.1f}â€¯MB)")
    print(f"Still avail : {gpu_avail/1024**2:8.1f}â€¯MB")

    # ---------------- CPU ----------------
    vm = psutil.virtual_memory()
    total_ram = vm.total
    avail_ram = vm.available

    proc = psutil.Process(os.getpid())
    rss  = proc.memory_info().rss          # Resident Set Size (bytes)

    print("\n======= CPU memory =========")
    print(f"Total RAM    : {total_ram/1024**3:6.2f}â€¯GB")
    print(f"Available RAM: {avail_ram/1024**3:6.2f}â€¯GB")
    print(f"Process RSS  : {rss/1024**2:6.1f}â€¯MB")
    print("-" * 45)

def print_gpu_usage(gpu_id: int = 0, where = None):
    props     = torch.cuda.get_device_properties(gpu_id)
    total     = props.total_memory
    reserved  = torch.cuda.memory_reserved(gpu_id)
    allocated = torch.cuda.memory_allocated(gpu_id)
    available = total - reserved

    if where != None:
        print(where,':')
    print(f"GPU {gpu_id} memory usage: {allocated/1024**2:.1f} MB, available: {available/1024**2:.1f} MB")

import pandas as pd

def inspect_simulator(sim):
    data = []
    for name, val in vars(sim).items():
        if isinstance(val, torch.Tensor):
            mem_bytes = val.numel() * val.element_size()
            device = str(val.device)
            dtype = str(val.dtype)
            shape = tuple(val.shape)
        elif hasattr(val, 'nbytes'):  # e.g., numpy arrays
            mem_bytes = val.nbytes
            device = 'CPU (numpy)'
            dtype = str(val.dtype)
            shape = tuple(val.shape)
        else:
            continue
        data.append({
            'Attribute': name,
            'Type': type(val).__name__,
            'Shape': shape,
            'Dtype': dtype,
            'Device': device,
            'Memory (MB)': mem_bytes / (1024**2)
        })
    df = pd.DataFrame(data)
    df = df.sort_values(by='Memory (MB)', ascending=False).reset_index(drop=True)
    return df


import numpy as np, torch

@torch.inference_mode()
def apply_reflective_echelon_beta01(
    A_in, sim, Hxy,
    *,
    n0=1.0,                  # é•œé¢æ‰€åœ¨ä»‹è´¨æŠ˜å°„ç‡ï¼ˆç©ºæ°”=1ï¼‰
    theta_deg=0.0,           # å…¥å°„è§’
    amplitude_r=1.0,         # å¹…åº¦åå°„ç³»æ•°ï¼ˆå¼ºåº¦R=|r|^2ï¼‰
    phi_const=np.pi,         # å¸¸æ•°åå°„ç›¸ä½
    omega_chunk: int = 1,    # æ¯æ¬¡å¤„ç†å¤šå°‘ä¸ªé¢‘ç‚¹(>=1)ï¼›1 æœ€çœæ˜¾å­˜ï¼Œ>1 æ›´å¿«
    tile_x: int | None = None  # æŒ‰ x æ–¹å‘åˆ‡ tileï¼›None=ä¸åˆ‡ï¼Œæ˜¾å­˜åƒç´§æ—¶å¯è®¾ 32/64/128
):
    """
    ä»…Î²0ä¸Î²1ä½œç”¨ï¼š
      Ã‚_out = r * Ã‚_in * exp{i[(Î²0 + Î²1Î©) L(x,y) + Ï†_const]},
    å…¶ä¸­ L(x,y) = 2 cosÎ¸ H(x,y), Î²0 = n0 Ï‰0 / c, Î²1 = n0 / c.

    ä½å†…å­˜å®ç°è¦ç‚¹ï¼š
      â€¢ ä¸æ„é€  3D ç›¸ä½ï¼›Î²0 ç”¨ 2D ç›¸ä½ä¸€æ¬¡æ€§ä¹˜ä¸Šï¼Œ
      â€¢ Î²1Â·Î© ç”¨â€œé¢‘ç‰‡å¾ªç¯â€ä¹˜ä¸Šï¼Œä»…ä¿ç•™ä¸€å¼  2D ä¸´æ—¶ç›¸ä½ã€‚
      â€¢ å³°å€¼æ˜¾å­˜ â‰ˆ A_w(1Ã—åœº) + A_after(1Ã—åœº) + O(Nx*Ny) 2D ä¸´æ—¶ã€‚
    """
    import math, torch
    dev = sim.T.device

    # 0) å‡†å¤‡è¾“å…¥åœºï¼ˆåœ¨ GPU / complex64ï¼‰
    A = (A_in if torch.is_tensor(A_in) else torch.as_tensor(A_in))
    A = A.to(dev, dtype=torch.complex64, non_blocking=True)
    
    # 1) è®¡ç®—å…‰ç¨‹ L(x,y)ï¼ˆ2D float32ï¼‰
    cos_theta = math.cos(math.radians(theta_deg))
    Lxy = (2.0 * cos_theta) * Hxy.to(dev, dtype=torch.float32, non_blocking=True)
    torch.cuda.empty_cache()
    # 2) é¢‘åŸŸåŒ–ï¼ˆå¾—åˆ° A_wï¼‰ï¼Œç«‹åˆ»é‡Šæ”¾æ—¶åŸŸ A ä»¥é™ä½å³°å€¼
    _, A_w = sim.fft_t(A)     # A_w: (Nx,Ny,Nt) complex64
    del A
    # ä¸å»ºè®®é¢‘ç¹ empty_cacheï¼Œä¼šæ‰“ä¹±ç¼“å­˜åˆ†é…ï¼›ç•™åˆ°å¤§æ­¥æ¸…ç†ç‚¹

    # 3) å…¨å±€å¹…åº¦ä¸å¸¸æ•°ç›¸ä½ï¼ˆæ ‡é‡ï¼‰
    #    r * e^{i phi_const}
    r_phase = complex(float(amplitude_r * math.cos(phi_const)),
                      float(amplitude_r * math.sin(phi_const)))
    A_w.mul_(r_phase)

    # 4) Î²0 Â· L(x,y) â€”â€” åªæ˜¯ä¸€å¼  2D ç›¸ä½å›¾
    beta0 = float(n0 * sim.omega0 / sim.c)   # [rad/m]
    phi0_xy = (beta0 * Lxy)                  # float32, (Nx,Ny)
    phase0_xy = torch.complex(torch.cos(phi0_xy), torch.sin(phi0_xy))  # complex64, (Nx,Ny)
    A_w.mul_(phase0_xy[..., None])           # åŸåœ°å¹¿æ’­ï¼Œä¸äº§ç”Ÿ 3D å‰¯æœ¬
    del phi0_xy, phase0_xy

    # 5) Î²1Â·L(x,y)Â·Î© â€”â€” æŒ‰é¢‘ç‰‡/å°å—å¤„ç†ï¼Œé¿å… 3D ç›¸ä½
    beta1 = float(n0 / sim.c)                # [s/m]
    omega = sim.omega.to(dev)                # (Nt,)

    Nx = sim.Nx
    Ny = sim.Ny
    tx = Nx if tile_x is None else int(tile_x)
    oc = max(1, int(omega_chunk))            # æ¯æ¬¡å¤„ç† oc ä¸ªé¢‘ç‚¹
    torch.cuda.empty_cache()

    for xs in range(0, Nx, tx):              # å¯é€‰ï¼šæŒ‰ x åˆ‡å—ï¼Œè¿›ä¸€æ­¥å‹å³°å€¼
        xe = min(xs + tx, Nx)
        L_blk = Lxy[xs:xe, :]                # (tx,Ny) float32

        if oc == 1:
            # â€”â€” æœ€çœæ˜¾å­˜ï¼šé€é¢‘ç‚¹å¤„ç† â€”â€” #
            for k in range(omega.numel()):
                theta_xy = -(beta1 * float(omega[k])) * L_blk       # (tx,Ny)
                phase_xy  = torch.complex(torch.cos(theta_xy),
                                           torch.sin(theta_xy))    # (tx,Ny) complex64
                A_w[xs:xe, :, k].mul_(phase_xy)                    # åŸåœ°ä¹˜
        else:
            # â€”â€” å°å—æŒ‰é¢‘ç‚¹æ‰¹é‡å¤„ç†ï¼ˆæ›´å¿«ï¼›æ˜¾å­˜â‰ˆ Nx*Ny*oc*å¤æ•°ï¼‰ â€”â€” #
            for k0 in range(0, omega.numel(), oc):
                k1 = min(k0 + oc, omega.numel())
                om = omega[k0:k1]                                  # (oc,)
                # æ„é€  (tx,Ny,oc) çš„å°ç›¸ä½å—
                theta = (beta1 * L_blk[..., None]) * om[None, None, :]     # float32
                phase = torch.complex(torch.cos(theta), torch.sin(theta))  # complex64
                A_w[xs:xe, :, k0:k1] *= phase
                del theta, phase

    del Lxy
    torch.cuda.empty_cache()

    # 6) å›åˆ°æ—¶åŸŸï¼ˆæ­¤åˆ»å³°å€¼â‰ˆ A_w + A_afterï¼›~2Ã—å®Œæ•´åœºï¼‰
    _, A_after = sim.ifft_t(A_w)
    # å¦‚éœ€ç»§ç»­é‡Šæ”¾ï¼Œå¯åœ¨è°ƒç”¨å¤„å† del A_w/empty_cache
    sim.A_out=None
    torch.cuda.empty_cache()
    return A_after.detach().cpu()



"""
 aperture_and_compress_full.py â€” GPUâ€‘friendly *fullâ€‘pulseâ€‘only* compression
 -----------------------------------------------------------------------------
 *2025â€‘07â€‘24 rewrite*
 
 â–¸ **What changed?**  The original implementation kept a full 3â€‘D electricâ€‘field
   tensor for *every* GDD point during the scan, which could blow up GPU memory
   when `N_scan` was large.  The new design:

   1.  Splits the task into two clear steps:
        â€¢ **`find_best_gdd()`** â€”â€”Â scan the GDD list, **never** stores the
          timeâ€‘domain field; only tracks the minimum TBP and its GDD.
        â€¢ **`compress_with_gdd()`** â€”â€”Â given a field & a single GDD, performs the
          compression once and *optionally* returns the compressed field.

   2.  `aperture_and_compress()` first calls `find_best_gdd()` (cheap & lean),
       then calls `compress_with_gdd()` *once* with `return_field=True` to get
       the final diagnostics / plots.

   3.  All helper maths (RMS width, FWHM, spectrumâ€¦) remain GPUâ€‘native Torch.

 Usage
 -----
 >>> sim.propagate()
 >>> from aperture_and_compress_full import aperture_and_compress
 >>> res = aperture_and_compress(sim, D_min=-3e-27, D_max=0.0, N_scan=401)

 The returned dict contains the optimal GDD, TBP, FWHM values and, by default,
 the compressed field tensor `A_comp`.  Set `return_field=False` in the public
 API if you only need the numbers.
 """

# -----------------------------------------------------------------------------
# 1. Torchâ€‘native helper metrics
# -----------------------------------------------------------------------------

def _rms_width(x: torch.Tensor, y: torch.Tensor):
    y = torch.abs(y)
    norm = torch.trapz(y, x)
    if norm == 0:
        return 0.0
    mean = torch.trapz(x * y, x) / norm
    var = torch.trapz((x - mean) ** 2 * y, x) / norm
    return torch.sqrt(var)



def _fwhm(x, y):
    """
    Fullâ€‘width at halfâ€‘maximum for a 1â€‘D profile.
    â€¢ Accepts torch.Tensor (CPU/CUDA) or np.ndarray.
    â€¢ è‡ªåŠ¨å¤„ç†â€œå³°å€¼åœ¨æ•°ç»„ä¸¤ç«¯â€çš„ FFT åŸç”Ÿæ’å¸ƒã€‚
    """
    import torch, numpy as np

    # --------- è½¬æˆ torch(cpu) ç»Ÿä¸€åç«¯ ----------
    if not torch.is_tensor(x):
        x = torch.as_tensor(x)
    if not torch.is_tensor(y):
        y = torch.as_tensor(y)
    if x.is_cuda: x = x.cpu()
    if y.is_cuda: y = y.cpu()

    y = torch.abs(y)
    if y.max() == 0:
        return 0.0

    half = 0.5 * y.max()
    idx  = torch.nonzero(y >= half, as_tuple=False).flatten()
    if idx.numel() < 2:
        return 0.0

    # --------- åˆ¤æ–­æ˜¯å¦â€œè·¨ç«¯â€ ----------------------
    wraps = (idx[0] == 0) and (idx[-1] == len(y) - 1)

    # -- è‹¥è·¨ç«¯ï¼ŒæŠŠæ•°ç»„ rollï¼Œè®©å³°å€¼åˆ°ä¸­å¿ƒï¼Œå†æ­£å¸¸è®¡ç®— --
    if wraps:
        peak = torch.argmax(y).item()          # é›¶é¢‘ or æœ€é«˜ç‚¹
        shift = len(y)//2 - peak
        y = torch.roll(y, shifts=shift, dims=0)
        x = torch.roll(x, shifts=shift, dims=0)
        idx  = torch.nonzero(y >= half, as_tuple=False).flatten()

    # --------- çº¿æ€§æ’å€¼å·¦å³åŠé«˜ç‚¹ ------------------
    def _interp(i_left):
        i_right = i_left + 1
        if i_right >= len(x):                  # é˜²æ­¢æœ«å°¾è¶Šç•Œ
            return x[i_left].item()
        x0, x1 = x[i_left],  x[i_right]
        y0, y1 = y[i_left],  y[i_right]
        return (x0 + (half - y0) * (x1 - x0) / (y1 - y0)).item()

    i0, i1 = idx[0], idx[-1]
    x_l = _interp(i0-1 if i0 > 0 else i0)
    x_r = _interp(i1)
    return x_r - x_l





# -----------------------------------------------------------------------------
# 2. Lowâ€‘level helpers (time profile & spectrum)
# -----------------------------------------------------------------------------

def _ensure_dev(x: torch.Tensor, dev):
    return x.to(dev, non_blocking=True) if x.device != dev else x

def _time_profile(A, sim):
    # æ—¶é—´æŠ•å½±åªåš sumï¼Œä¸è¦æ±‚åœ¨ GPU ä¸Š
    A = _ensure_dev(A, sim.T.device) 
    I_t=torch.sum(torch.abs(A)**2, dim=(0,1)) * sim.dx * sim.dy
    return I_t

def _spectrum(A, sim):
    A = _ensure_dev(A, sim.T.device)     # ç¡®ä¿ä¸ sim.fft_t çš„ç¼©æ”¾å¼ é‡åŒè®¾å¤‡

    _, A_w = sim.fft_t(A)
    del A
    torch.cuda.empty_cache()
    S_w = torch.sum(torch.abs(A_w)**2, dim=(0,1)) * sim.dx * sim.dy
    return S_w.detach().cpu().numpy()



# -----------------------------------------------------------------------------
# 3. Core utility â€“ compress once with a given GDD
# -----------------------------------------------------------------------------

@torch.inference_mode()
def compress_with_gdd(A_out, sim, D, *, return_field=True):
    """Compress a field with secondâ€‘order dispersion *D*.

    Parameters
    ----------
    A_gpu        : (Nx,Ny,Nt) complex Torch tensor (on the same device as *sim*)
    sim          : PulseGNLSESimulator instance (provides grids & FFT helpers)
    D            : float, groupâ€‘delay dispersion [sÂ²]
    return_field : bool, if *True* include the compressed field tensor in the
                   returned dict; otherwise memoryâ€‘cheap.

    Returns
    -------
    dict
        tbp        : timeâ€“bandwidth product
        fwhm_t     : FWHM in time (s)
        sigma_w    : RMS bandwidth (rad/s)
        A_comp     : compressed field (Torch tensor) â€“ only if *return_field*
    """
    Ï‰ = sim.omega
    
    dx, dy = sim.dx, sim.dy

    A_gpu = A_out.to(sim.T.device)

    '''
    phase = torch.exp(0.5j * D * Ï‰ ** 2)
    _, A_w = sim.fft_t(A_gpu)
    print_gpu_usage(where='A_w')
    A_w_D = A_w * phase[None, None, :]
    print_gpu_usage(where='A_w_D')

    # --- metrics in the spectral domain ------------------------------------
    sigma_w = _rms_width(Ï‰, torch.sum(torch.abs(A_w_D) ** 2, dim=(0, 1)) * dx * dy)
    print_gpu_usage(where='sigma_w')

    # --- back to time domain to get temporal metrics -----------------------
    _, A_t = sim.ifft_t(A_w_D)
    print_gpu_usage(where='A_t')
    P_t = _time_profile(A_t, sim)
    '''
    A_w_D =  sim.fft_t(A_gpu)[1]
    del A_gpu
    torch.cuda.empty_cache()
    A_w_D = A_w_D* torch.exp(0.5j * D *Ï‰ ** 2)[None, None, :]
    torch.cuda.empty_cache()
    I_w_D=torch.sum(torch.abs(A_w_D) ** 2, dim=(0, 1)) 

    I_w_D=I_w_D * dx * dy
    
    sigma_w = _rms_width(Ï‰,I_w_D)

    A_t_D=sim.ifft_t(A_w_D)[1]
    del A_w_D
    torch.cuda.empty_cache()
    
    P_t = _time_profile(A_t_D, sim)

    sigma_t = _rms_width(sim.T, P_t)

    fwhm_t = _fwhm(sim.T, P_t)

    tbp = (sigma_t * sigma_w).item()

    out = dict(tbp=tbp, fwhm_t=fwhm_t, sigma_w=sigma_w.item())

    if return_field:
        out["A_comp"] = A_t_D.detach().cpu()   # â† æ”¾åˆ° CPU

    return out


# -----------------------------------------------------------------------------
# 4. Fast scan â€“ find the best GDD without keeping fields
# -----------------------------------------------------------------------------

def find_best_gdd(A_out, sim, D_scan):
    """Return the GDD in *D_scan* that minimises the TBP.

    Only TBP & D are tracked â€“ *no* field tensor is kept, saving memory.
    """
    best_tbp = float("inf")
    best_D = D_scan[0]
    tbp_curve = []
    flag=0
    for D in D_scan:
        #print(flag)
        flag+=1
        res = compress_with_gdd(A_out, sim, D, return_field=False)
        tbp_curve.append(res["tbp"])
        if res["tbp"] < best_tbp:
            best_tbp = res["tbp"]
            best_D = D

    return best_D, best_tbp, tbp_curve


# -----------------------------------------------------------------------------
# 5. Public API â€“ highâ€‘level helper (scan + one compression)
# -----------------------------------------------------------------------------
import math
import torch

@torch.inference_mode()
def design_circular_aperture_for_fraction(
    A_field: torch.Tensor, sim,
    *,
    pass_fraction: float,       # 0< f â‰¤1ï¼Œå¦‚ 0.85 è¡¨ç¤ºé€šè¿‡ 85% èƒ½é‡
    fwhm: float = 40e-6,        # è½¯è¾¹ FWHMï¼ˆç±³ï¼‰
    tol: float = 1e-3,          # é€šè¿‡èƒ½é‡çš„ç›¸å¯¹è¯¯å·®å®¹å¿
    max_iter: int = 40
):
    """
    ä¾æ®ç›®æ ‡é€šè¿‡èƒ½é‡æ¯”ä¾‹ï¼Œåœ¨ x-y å¹³é¢ä¸º A_field è®¾è®¡ä¸€ä¸ªåœ†å½¢è½¯è¾¹å…‰é˜‘ï¼š
        M_R(x,y) = soft_unit_step(R - r, fwhm)
    å…¶ä¸­ r = sqrt(x^2 + y^2)ï¼ŒR ç”±äºŒåˆ†æ³•è‡ªåŠ¨æ±‚å¾—ï¼Œä½¿ï¼š
        frac(R) = âˆ¬ I_xy M_R dxdy / âˆ¬ I_xy dxdy  â‰ˆ pass_fraction

    è¿”å›ï¼š
      M_xy       : (Nx,Ny) float32 âˆˆ[0,1] çš„è½¯è¾¹å…‰é˜‘æ©è†œï¼ˆåœ¨ sim.T.device ä¸Šï¼‰
      R_opt      : æœ€ç»ˆåŠå¾„ï¼ˆç±³ï¼‰
      frac_ach   : å®é™…é€šè¿‡æ¯”ä¾‹
    """
    assert 0.0 < pass_fraction <= 1.0, "pass_fraction å¿…é¡»åœ¨ (0,1]"

    dev = sim.T.device
    # â€”â€” I_xy = âˆ« |A|^2 dt â€”â€”ï¼ˆdxÂ·dy ä¼šåœ¨æ¯”å€¼ä¸­ç›¸æ¶ˆï¼‰
    A = (A_field if torch.is_tensor(A_field) else torch.as_tensor(A_field)).to(dev, dtype=torch.complex64, non_blocking=True)
    I_xy = torch.sum(A.real*A.real + A.imag*A.imag, dim=2) * float(sim.dT)   # (Nx,Ny) float32
    del A; torch.cuda.empty_cache()

    denom = torch.sum(I_xy)  # âˆ¬ I_xy dxdy ä¸­ dxdy çœç•¥ï¼Œåšæ¯”ä¾‹ä¼šç›¸æ¶ˆ
    if denom.item() <= 0:
        # ç©ºèƒ½é‡å…œåº•ï¼šä¸ç»™å…‰é˜‘
        M_full = torch.ones((sim.Nx, sim.Ny), device=dev, dtype=torch.float32)
        return M_full, float(min(sim.x.abs().max(), sim.y.abs().max())), 1.0

    # åŠå¾„ç½‘æ ¼
    r = torch.sqrt(sim.X.to(dev)**2 + sim.Y.to(dev)**2)  # (Nx,Ny)

    # ç›®æ ‡å‡½æ•°ï¼šç»™å®š R â†’ é€šè¿‡æ¯”ä¾‹
    def frac_of_R(R: float) -> float:
        M = soft_unit_step(R - r, fwhm)      # å†…éƒ¨â‰ˆ1ï¼Œå¤–éƒ¨â‰ˆ0
        num = torch.sum(I_xy * M)
        return float((num / denom).item())

    # ç‰¹ä¾‹ï¼šâ‰ˆ1 å°±ç›´æ¥æ»¡ç‰‡
    if pass_fraction >= 1.0 - 1e-6:
        M_full = soft_unit_step((r.max().item()*2) - r, fwhm)  # åŸºæœ¬ä¸Šå…¨ 1
        return M_full, float(r.max().item()), 1.0

    # äºŒåˆ†æ³•æ±‚ R
    lo = 0.0
    hi = float(r.max().item())
    frac_lo = frac_of_R(lo)  # â‰ˆ0
    frac_hi = frac_of_R(hi)  # â‰¤1

    # ä¿æŠ¤ï¼šè‹¥ hi ä» < ç›®æ ‡ï¼ˆæ•°å€¼è¾¹ç¼˜ï¼‰ï¼Œå°±å– hi
    if frac_hi < pass_fraction:
        M_hi = soft_unit_step(hi - r, fwhm)
        return M_hi, hi, frac_hi

    R = hi
    frac_mid = frac_hi
    for _ in range(max_iter):
        mid = 0.5*(lo + hi)
        fmid = frac_of_R(mid)
        if fmid < pass_fraction:
            lo = mid
        else:
            hi = mid
        R = hi
        frac_mid = fmid
        if abs(fmid - pass_fraction) <= tol * pass_fraction:
            break

    M_xy = soft_unit_step(R - r, fwhm).to(torch.float32)
    return M_xy, R, frac_mid


def aperture_and_compress(
    sim,
    *,
    D_min=-3e-27,
    D_max=0.0,
    N_scan=401,
    plot=True,
    return_field=True,
    # ===== æ–°å¢ï¼šç©ºé—´å…‰é˜‘é€‰é¡¹ =====
    aperture_pass_fraction: float | None = None,   # ä¼  None/ä¸ä¼  â‡’ ä¸åŠ å…‰é˜‘
    aperture_fwhm: float = 40e-6,                  # è½¯è¾¹å®½åº¦ï¼ˆç±³ï¼‰
    return_aperture_mask: bool = False,            # æ˜¯å¦æŠŠå…‰é˜‘æ©è†œä¹Ÿæ”¾è¿›ç»“æœ
):
    """
    å…ˆï¼ˆå¯é€‰ï¼‰åŠ ç©ºé—´å…‰é˜‘ï¼Œå†æ‰«æ GDD å‹ç¼©ã€‚
      â€¢ aperture_pass_fraction=None    â‡’ ä¸åŠ å…‰é˜‘ï¼ˆä¸åŸç‰ˆå®Œå…¨ä¸€è‡´ï¼‰
      â€¢ aperture_pass_fractionâˆˆ(0,1]  â‡’ è‡ªåŠ¨æ±‚å…‰é˜‘åŠå¾„ï¼Œä½¿é€šè¿‡èƒ½é‡â‰ˆè¯¥æ¯”ä¾‹
    """
    if sim.A_out is None:
        raise ValueError("sim.A_out is None â€” run sim.propagate() first.")
    A_out = sim.A_out   # CPU å¼ é‡
    sim.A_out=None
    torch.cuda.empty_cache()

    # ---------- (A) å¯é€‰ï¼šç©ºé—´å…‰é˜‘ ----------
    aperture_info = None
    if aperture_pass_fraction is not None:
        # è®¾è®¡å…‰é˜‘ï¼ˆåœ¨ GPU ä¸Šç®—ï¼‰ï¼Œå¹¶åº”ç”¨åˆ°åœº
        M_xy, R_opt, frac_ach = design_circular_aperture_for_fraction(
            A_out, sim,
            pass_fraction=float(aperture_pass_fraction),
            fwhm=float(aperture_fwhm),
            tol=1e-3, max_iter=40,
        )
        A_base_gpu = (A_out if torch.is_tensor(A_out) else torch.as_tensor(A_out)).to(sim.T.device, dtype=torch.complex64)
        A_base_gpu = A_base_gpu * M_xy[..., None].to(A_base_gpu.dtype)   # åº”ç”¨å…‰é˜‘
        A_base = A_base_gpu.detach().cpu()
        del A_base_gpu
        torch.cuda.empty_cache()

        aperture_info = dict(
            pass_target=float(aperture_pass_fraction),
            pass_achieved=float(frac_ach),
            radius_m=float(R_opt),
            fwhm_m=float(aperture_fwhm),
        )
    else:
        # ä¸åŠ å…‰é˜‘ â‡’ ä½¿ç”¨åŸæ¥çš„è¾“å‡ºåœº
        A_base = A_out

    # ---------- (B) ç²—æ‰« GDDï¼ˆå¯¹ A_baseï¼‰ ----------
    D_scan = np.linspace(D_min, D_max, N_scan)
    D_best, tbp_min, tbp_curve = find_best_gdd(A_base, sim, D_scan)
    
    sigma_w_init = _rms_width(
        sim.omega,
        torch.sum(torch.abs(sim.fft_t(sim.A_in.to(sim.T.device, non_blocking=True))[1]) ** 2, dim=(0, 1)) * sim.dx * sim.dy,
    )

    if plot:
        S_w_in   = _spectrum(sim.A_in, sim);     S_w_in_c   = np.fft.fftshift(S_w_in)
        P_t_in   = _time_profile(sim.A_in, sim).cpu().numpy()
        sim.A_in=None
    
    torch.cuda.empty_cache()
    # ---------- (C) æœ€ç»ˆå‹ç¼© ----------
    final = compress_with_gdd(A_base, sim, D_best, return_field=return_field)
    torch.cuda.empty_cache()
    check_mem()
    # ---------- (D) æ§åˆ¶å°æ‘˜è¦ ----------

    print(
        f"FWHM_t(best): {final['fwhm_t']*1e15:.2f} fs  "
        f"(GDD={D_best:.3e} sÂ²,  TBP={final['tbp']:.3f})"
    )
    if aperture_info is not None:
        print(f"[Aperture] target={aperture_info['pass_target']*100:.1f}% "
              f"â†’ achieved={aperture_info['pass_achieved']*100:.1f}% , "
              f"Râ‰ˆ{aperture_info['radius_m']*1e3:.2f} mm,  FWHM={aperture_info['fwhm_m']*1e6:.1f} Âµm")

    # ---------- (E) å¯é€‰ç»˜å›¾ï¼ˆæŠŠâ€œOutputâ€åŸºå‡†æ¢æˆ A_baseï¼‰ ----------
    if plot:
        omega_c = torch.fft.fftshift(sim.omega).cpu().numpy()
        freq_THz_c = omega_c / (2*np.pi*1e12)

        # é¢‘è°±ï¼šè¾“å…¥ vsï¼ˆå…‰é˜‘åï¼‰è¾“å‡º vs å‹ç¼©å
        S_w_base = _spectrum(A_base,   sim);     S_w_base_c = np.fft.fftshift(S_w_base)
        S_w_comp_c = None
        if return_field:
            S_w_comp = _spectrum(final["A_comp"], sim)
            S_w_comp_c = np.fft.fftshift(S_w_comp)

        # æ—¶é—´åŸŸ
        T_fs = (sim.T * 1e15).cpu().numpy()
        P_t_base = _time_profile(A_base,   sim).cpu().numpy()


        
        P_t_comp = None
        if return_field:
            P_t_comp = _time_profile(final["A_comp"], sim).cpu().numpy()

        fwhm_in = _fwhm(sim.T, torch.as_tensor(P_t_in))
        half_tr_fs = min(5 * fwhm_in * 1e15, abs(T_fs[-1]))

        # é¢‘åŸŸè§†é‡
        half_fr = 2 * _fwhm(torch.as_tensor(omega_c), torch.as_tensor(S_w_in_c)) / (2*np.pi*1e12)

        fig, axes = plt.subplots(4, 1, figsize=(6, 12))

        # (0) æ—¶é—´åŸŸï¼šè¾“å…¥ / ï¼ˆå…‰é˜‘åï¼‰è¾“å‡º / å‹ç¼©å
        axes[0].plot(T_fs, P_t_in,  label="Input")
        axes[0].plot(T_fs, P_t_base,label="Output (after aperture)" if aperture_info else "Output")
        if P_t_comp is not None:
            axes[0].plot(T_fs, P_t_comp, ls="--", label="Compressed")
        axes[0].set_xlim(-half_tr_fs, half_tr_fs)
        axes[0].set(xlabel="Time [fs]", ylabel="âˆ«|A|Â² dx dy  [J/s]",
                    title="Temporal power")
        axes[0].legend(); axes[0].grid()

        # (1) é¢‘è°±ï¼šè¾“å…¥ / ï¼ˆå…‰é˜‘åï¼‰è¾“å‡º / å‹ç¼©åï¼ˆå½’ä¸€åŒ–å¯¹æ¯”ï¼‰
        axes[1].plot(freq_THz_c, S_w_in_c   / max(S_w_in_c.max(), 1e-30), label="Before (input)")
        axes[1].plot(freq_THz_c, S_w_base_c / max(S_w_base_c.max(), 1e-30),
                     label="After aperture" if aperture_info else "Before (output)")
        if S_w_comp_c is not None:
            axes[1].plot(freq_THz_c, S_w_comp_c / max(S_w_comp_c.max(), 1e-30),
                         ls="--", label="After compression")
        axes[1].set_xlim(-half_fr, half_fr)
        axes[1].set(xlabel="Frequency offset [THz]", ylabel="Norm. power", title="Spectrum")
        axes[1].legend(); axes[1].grid()

        # (2) TBP-GDD æ‰«æ
        axes[2].plot(D_scan*1e28, tbp_curve)
        axes[2].scatter(D_best*1e28, tbp_min, color="red")
        axes[2].axhline(0.5, ls="--", color="gray")
        axes[2].set(xlabel="GDD [Ã—10â»Â²â¸ sÂ²]", ylabel="TBP", title="GDD scan")
        axes[2].grid()

        # (3) å‹ç¼©å‰åæ—¶é—´åŸŸï¼ˆä»¥â€œå…‰é˜‘åè¾“å‡ºâ€ä¸ºåŸºå‡†ï¼‰
        P_t_before = _time_profile(A_base, sim).cpu().numpy()
        axes[3].plot(T_fs, P_t_before, label="Before (after aperture)" if aperture_info else "Before")
        if return_field:
            P_t_after = _time_profile(final["A_comp"], sim).cpu().numpy()
            axes[3].plot(T_fs, P_t_after, ls="--", label="After")
        axes[3].set(xlabel="Time [fs]", ylabel="âˆ«|A|Â² dx dy  [J/s]",
                    title="Temporal profile (before vs after)")
        axes[3].legend(); axes[3].grid()

        fig.tight_layout(); plt.show()

    # ---------- (F) ç»“æœ ----------
    result = dict(D_opt=D_best, tbp=final["tbp"], fwhm_t=final["fwhm_t"],
                  sigma_w=final["sigma_w"], tbp_curve=tbp_curve)
    if return_field:
        result["A_comp"] = final["A_comp"]
    if aperture_info is not None:
        result["aperture"] = aperture_info
        if return_aperture_mask:
            # è¿”å› CPU ç‰ˆæ©è†œï¼Œä¾¿äºå¤–éƒ¨å¤ç”¨/ä¿å­˜
            result["aperture"]["mask_xy"] = M_xy.detach().cpu()
    sim.A_out= A_out
    return result


import os, torch, gc

def save_A_every_step(step, sim, save_dir="A_dump", every=1):
    if step % every:                 # åªåœ¨ every çš„å€æ•°æ­¥ä¿å­˜
        return
    os.makedirs(save_dir, exist_ok=True)
    fname = os.path.join(save_dir, f"A_{step:05d}.pt")
    # ç›´æ¥åºåˆ—åŒ– GPU tensorï¼›torch ä¼šåœ¨å†…éƒ¨æ¬åˆ° CPU å¹¶å†™æ–‡ä»¶
    torch.save(sim.A.detach(), fname)
    # æ˜¾å¼æ¸…ç† CPU bufferï¼Œå‡è½» host å†…å­˜å ç”¨
    gc.collect()

# ========= quick diagnostics to print before every visualization =========
@torch.inference_mode()
def diagnose_and_print(sim, *, field="current", label=None):
    """
    field: 'input' | 'current' | 'output'
    æ‰“å°ï¼šEnergy, Peak power, FWHM/Ïƒ_t, FWHM/Ïƒ_v, FWHM_x/y, t_peak, t_centroid, B-integral
    """
    dev = sim.T.device
    if field == "input":
        A_src = sim.A_in
    elif field == "output":
        if sim.A_out is None:
            raise ValueError("sim.A_out is None â€” run propagate() first")
        A_src = sim.A_out
    else:
        A_src = sim.A

    with torch.no_grad():
        A_gpu = (A_src if torch.is_tensor(A_src) else torch.from_numpy(A_src)).to(dev, non_blocking=True)
        # ---- Energy & temporal power ----
        torch.cuda.empty_cache()
        I=torch.abs(A_gpu)**2
        I_t=torch.trapz(I, sim.T)
        E = torch.trapz(torch.trapz(I_t, sim.y), sim.x).item()

        # ç©ºé—´ FWHMï¼ˆå– yâ‰ˆ0 / xâ‰ˆ0 çš„çº¿ï¼‰
        I_xy = torch.trapz(I, sim.T, dim=2)

        P_t = torch.sum(I, dim=(0, 1)) * sim.dx * sim.dy
        del I
        torch.cuda.empty_cache()
        P_peak = P_t.max().item()

        # æ—¶é—´åŸŸæŒ‡æ ‡
        sigma_t = _rms_width(sim.T, P_t).item()
        fwhm_t  = _fwhm(sim.T, P_t)
        t_peak  = sim.T[torch.argmax(P_t)].item() if P_peak > 0 else 0.0
        denom   = torch.trapz(P_t, sim.T)
        t_cm    = (torch.trapz(sim.T * P_t, sim.T) / denom).item() if float(denom) != 0.0 else 0.0

        # é¢‘åŸŸæŒ‡æ ‡
        _, A_w = sim.fft_t(A_gpu)
        A_w =A_w.detach().cpu().numpy()
        I_w= np.sum(np.abs(A_w)**2, axis=(0, 1))
        I_w=torch.from_numpy(I_w).to(dev)
        S_w = I_w * sim.dx * sim.dy
        sigma_w = _rms_width(sim.omega, S_w).item()
        fwhm_w  = _fwhm(sim.omega, S_w)

        # ç©ºé—´ FWHMï¼ˆå– yâ‰ˆ0 / xâ‰ˆ0 çš„çº¿ï¼‰
        iy0 = torch.argmin(torch.abs(sim.y)).item()
        ix0 = torch.argmin(torch.abs(sim.x)).item()
        fwhm_x = float(_fwhm(sim.x, I_xy[:, iy0]))
        fwhm_y = float(_fwhm(sim.y, I_xy[ix0,  :]))

        B = getattr(sim, "B_running", float("nan"))

    name = label or field.upper()
    print(f"\n===== Diagnostics [{name}] =====")
    print(f"Energy                : {E:.4e} J")
    print(f"Peak power            : {P_peak/1e6:.2f} MW")
    print(f"Time  â€” FWHM / Ïƒ_t    : {fwhm_t*1e15:.2f} fs / {sigma_t*1e15:.2f} fs")
    print(f"Freq  â€” FWHM / Ïƒ_v    : {fwhm_w/2/np.pi/1e12:.2f} THz / {sigma_w/2/np.pi/1e12:.2f} THz")
    print(f"Space â€” FWHM_x / FWHM_y: {fwhm_x*1e3:.2f} mm / {fwhm_y*1e3:.2f} mm")
    print(f"Centers (t_peak / t_cm): {t_peak*1e15:.2f} fs / {t_cm*1e15:.2f} fs")
    if field != "input":
        print(f"B-integral (running)  : {B:.3f} (approx.)")
    print("================================\n")

# ========= 2) æ‰«æï¼šèƒ½é‡é€šè¿‡æ¯”ä¾‹ 100% â†’ 10%ï¼Œåªè®°å½•å‹ç¼©åçš„ FWHM =========
import numpy as np
import matplotlib.pyplot as plt

def sweep_energy_clip_and_plot(sim,
                               passes=None,             # èƒ½é‡é€šè¿‡æ¯”ä¾‹åˆ—è¡¨ï¼ˆ0~1ï¼‰
                               D_min=-3e-28, D_max=0.0, # GDD æ‰«æèŒƒå›´
                               N_scan=201,              # GDD æ‰«æç‚¹æ•°
                               aperture_fwhm=30e-6,     # è½¯è¾¹å®½åº¦
                               save=True,               # æ˜¯å¦ä¿å­˜ç»“æœ
                               save_dir=None,           # ä¿å­˜ç›®å½•ï¼›é»˜è®¤ç”¨å…¨å±€ BASE_SAVE
                               filename_prefix="fwhm_vs_energy_after_mirror"  # æ–‡ä»¶åå‰ç¼€
                               ):
    """
    å¯¹æ¯ä¸ªèƒ½é‡é€šè¿‡æ¯”ä¾‹ pass_fractionï¼š
      1) è‡ªåŠ¨è®¾è®¡åœ†å½¢è½¯è¾¹å…‰é˜‘ï¼ˆé€šè¿‡æ¯”ä¾‹=pass_fractionï¼‰
      2) æ‰«æ GDDï¼Œæ‰¾åˆ° TBP æœ€å°å¤„
      3) è®°å½•å¯¹åº”çš„å‹ç¼©å FWHM_tï¼ˆç§’ â†’ fsï¼‰
    ä¸è¿”å›å‹ç¼©åœºï¼Œä»…è¿”å›æ•°å€¼å¹¶ç”»å›¾ï¼›å¯å°† (x,y) æ•°æ®ä¸å›¾ä¿å­˜åˆ° BASE_SAVEã€‚
    """
    import os, time
    if passes is None:
        # 100%â†’10%ï¼Œæ­¥é•¿ 2%
        passes = [p/100 for p in range(100, 9, -4)]

    fwhm_fs = []
    for pf in passes:
        res = aperture_and_compress(
            sim,
            D_min=D_min, D_max=D_max, N_scan=N_scan,
            plot=False,             # ä¸ç”»æ¯æ¬¡å‹ç¼©çš„å›¾
            return_field=False,     # ä¸è¿”å›å‹ç¼©åçš„åœºï¼ŒèŠ‚çœå†…å­˜
            aperture_pass_fraction=pf,
            aperture_fwhm=aperture_fwhm,
            return_aperture_mask=False
        )
        fwhm_fs.append(res["fwhm_t"] * 1e15)   # s â†’ fs

        # å¯é€‰ï¼šé‡Šæ”¾ cuFFT è®¡åˆ’ä¸ç¼“å­˜ï¼ˆå¦‚æœä½ æœ‰ release_fft_poolï¼‰
        try:
            release_fft_pool()
        except Exception:
            torch.cuda.empty_cache()

    # ========= 3) ç”»å›¾ï¼šèƒ½é‡é€šè¿‡æ¯”ä¾‹(%) vs å‹ç¼©å FWHM(fs) =========
    x_pct = [p*100 for p in passes]
    fig = plt.figure(figsize=(6,4))
    ax = fig.add_subplot(111)
    ax.plot(x_pct, fwhm_fs, marker='o')
    ax.set_xlabel("Energy pass [%]")            # èƒ½é‡å‰ªè£ç†è§£ä¸ºâ€œä¿ç•™ç™¾åˆ†æ¯”â€
    ax.set_ylabel("Compressed FWHM [fs]")
    ax.set_title("FWHM vs Energy Pass")
    ax.grid(True)
    # å¦‚éœ€â€œå‰ªå¾—è¶Šç‹ åœ¨å³ä¾§â€ï¼Œå–æ¶ˆæ³¨é‡Šä¸‹ä¸€è¡Œ
    # ax.invert_xaxis()
    fig.tight_layout()
    plt.show()

    # ========= 4) ä¿å­˜åˆ° BASE_SAVEï¼ˆæ•°æ® + å›¾ï¼‰ =========
    if save:
        out_dir = save_dir or (BASE_SAVE if 'BASE_SAVE' in globals() else ".")
        os.makedirs(out_dir, exist_ok=True)
        # æ—¶é—´æˆ³é¿å…è¦†ç›–
        ts = time.strftime("%Y%m%d_%H%M%S")
        csv_path = os.path.join(out_dir, f"{filename_prefix}_{ts}.csv")
        npz_path = os.path.join(out_dir, f"{filename_prefix}_{ts}.npz")
        fig_path = os.path.join(out_dir, f"{filename_prefix}_{ts}.png")

        # ä¿å­˜ CSVï¼ˆä¸¤åˆ—ï¼šenergy_pass_pct, fwhm_fsï¼‰
        arr = np.column_stack([np.array(x_pct, dtype=float),
                               np.array(fwhm_fs, dtype=float)])
        np.savetxt(csv_path, arr, delimiter=",",
                   header="energy_pass_pct,fwhm_fs", comments="")
        # ä¿å­˜ NPZï¼ˆåŸå§‹ pass âˆˆ[0,1] ä¹Ÿä¸€å¹¶æ”¾è¿›å»ï¼‰
        np.savez(npz_path,
                 energy_pass=np.array(passes, dtype=float),
                 energy_pass_pct=np.array(x_pct, dtype=float),
                 fwhm_fs=np.array(fwhm_fs, dtype=float))
        # ä¿å­˜å›¾
        fig.savefig(fig_path, dpi=200, bbox_inches="tight")
        print(f"[saved] CSV: {csv_path}\n[saved] NPZ: {npz_path}\n[saved] FIG: {fig_path}")

    return np.array(passes), np.array(fwhm_fs)
